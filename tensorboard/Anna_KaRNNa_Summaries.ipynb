{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  9, 51, 77,  3, 18, 10, 37, 71, 11, 11, 11, 56, 51, 77, 77, 79,\n",
       "       37, 25, 51, 30, 62, 41, 62, 18, 53, 37, 51, 10, 18, 37, 51, 41, 41,\n",
       "       37, 51, 41, 62, 35, 18, 82, 37, 18, 38, 18, 10, 79, 37,  2, 73,  9,\n",
       "       51, 77, 77, 79, 37, 25, 51, 30, 62, 41, 79, 37, 62, 53, 37,  2, 73,\n",
       "        9, 51, 77, 77, 79, 37, 62, 73, 37, 62,  3, 53, 37, 23, 81, 73, 11,\n",
       "       81, 51, 79, 12, 11, 11, 65, 38, 18, 10, 79,  3,  9, 62, 73], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9, 51, 77,  3, 18, 10, 37, 71, 11],\n",
       "       [54, 73, 46, 37,  9, 18, 37, 30, 23, 38],\n",
       "       [37, 63, 51,  3, 63,  9, 62, 73, 13, 37],\n",
       "       [23,  3,  9, 18, 10, 37, 81, 23,  2, 41],\n",
       "       [37,  3,  9, 18, 37, 41, 51, 73, 46,  0],\n",
       "       [37, 69,  9, 10, 23,  2, 13,  9, 37, 41],\n",
       "       [ 3, 37,  3, 23, 11, 46, 23, 12, 11, 11],\n",
       "       [23, 37,  9, 18, 10, 53, 18, 41, 25,  4],\n",
       "       [ 9, 51,  3, 37, 62, 53, 37,  3,  9, 18],\n",
       "       [18, 10, 53, 18, 41, 25, 37, 51, 73, 46]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        # my own experiment:\n",
    "#        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "#                               name='softmax_w')\n",
    "        softmax_w = tf.Variable(tf.random_uniform((lstm_size, num_classes), name='softmax_w'))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 10\n",
    "lstm_size = 10\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/1786 Training loss: 4.4191 0.0591 sec/batch\n",
      "Epoch 1/1  Iteration 2/1786 Training loss: 4.4171 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 3/1786 Training loss: 4.4151 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 4/1786 Training loss: 4.4134 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 5/1786 Training loss: 4.4122 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 6/1786 Training loss: 4.4102 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 7/1786 Training loss: 4.4088 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 8/1786 Training loss: 4.4077 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 9/1786 Training loss: 4.4062 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 10/1786 Training loss: 4.4046 0.0412 sec/batch\n",
      "Epoch 1/1  Iteration 11/1786 Training loss: 4.4032 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 12/1786 Training loss: 4.4014 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 13/1786 Training loss: 4.4000 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 14/1786 Training loss: 4.3984 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 15/1786 Training loss: 4.3967 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 16/1786 Training loss: 4.3950 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 17/1786 Training loss: 4.3935 0.0389 sec/batch\n",
      "Epoch 1/1  Iteration 18/1786 Training loss: 4.3918 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 19/1786 Training loss: 4.3900 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 20/1786 Training loss: 4.3883 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 21/1786 Training loss: 4.3864 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 22/1786 Training loss: 4.3844 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 23/1786 Training loss: 4.3823 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 24/1786 Training loss: 4.3804 0.0457 sec/batch\n",
      "Epoch 1/1  Iteration 25/1786 Training loss: 4.3781 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 26/1786 Training loss: 4.3759 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 27/1786 Training loss: 4.3736 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 28/1786 Training loss: 4.3715 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 29/1786 Training loss: 4.3690 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 30/1786 Training loss: 4.3665 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 31/1786 Training loss: 4.3637 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 32/1786 Training loss: 4.3609 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 33/1786 Training loss: 4.3581 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 34/1786 Training loss: 4.3549 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 35/1786 Training loss: 4.3515 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 36/1786 Training loss: 4.3484 0.0356 sec/batch\n",
      "Epoch 1/1  Iteration 37/1786 Training loss: 4.3452 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 38/1786 Training loss: 4.3422 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 39/1786 Training loss: 4.3388 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 40/1786 Training loss: 4.3350 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 41/1786 Training loss: 4.3314 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 42/1786 Training loss: 4.3274 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 43/1786 Training loss: 4.3238 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 44/1786 Training loss: 4.3193 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 45/1786 Training loss: 4.3146 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 46/1786 Training loss: 4.3100 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 47/1786 Training loss: 4.3055 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 48/1786 Training loss: 4.3010 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 49/1786 Training loss: 4.2962 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 50/1786 Training loss: 4.2914 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 51/1786 Training loss: 4.2867 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 52/1786 Training loss: 4.2818 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 53/1786 Training loss: 4.2768 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 54/1786 Training loss: 4.2718 0.0340 sec/batch\n",
      "Epoch 1/1  Iteration 55/1786 Training loss: 4.2672 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 56/1786 Training loss: 4.2617 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 57/1786 Training loss: 4.2562 0.0324 sec/batch\n",
      "Epoch 1/1  Iteration 58/1786 Training loss: 4.2500 0.0369 sec/batch\n",
      "Epoch 1/1  Iteration 59/1786 Training loss: 4.2448 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 60/1786 Training loss: 4.2396 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 61/1786 Training loss: 4.2348 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 62/1786 Training loss: 4.2303 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 63/1786 Training loss: 4.2256 0.0344 sec/batch\n",
      "Epoch 1/1  Iteration 64/1786 Training loss: 4.2205 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 65/1786 Training loss: 4.2159 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 66/1786 Training loss: 4.2117 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 67/1786 Training loss: 4.2071 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 68/1786 Training loss: 4.2024 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 69/1786 Training loss: 4.1977 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 70/1786 Training loss: 4.1931 0.0380 sec/batch\n",
      "Epoch 1/1  Iteration 71/1786 Training loss: 4.1877 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 72/1786 Training loss: 4.1832 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 73/1786 Training loss: 4.1782 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 74/1786 Training loss: 4.1732 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 75/1786 Training loss: 4.1687 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 76/1786 Training loss: 4.1642 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 77/1786 Training loss: 4.1594 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 78/1786 Training loss: 4.1546 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 79/1786 Training loss: 4.1503 0.0326 sec/batch\n",
      "Epoch 1/1  Iteration 80/1786 Training loss: 4.1460 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 81/1786 Training loss: 4.1410 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 82/1786 Training loss: 4.1363 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 83/1786 Training loss: 4.1321 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 84/1786 Training loss: 4.1275 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 85/1786 Training loss: 4.1236 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 86/1786 Training loss: 4.1187 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 87/1786 Training loss: 4.1147 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 88/1786 Training loss: 4.1105 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 89/1786 Training loss: 4.1064 0.0359 sec/batch\n",
      "Epoch 1/1  Iteration 90/1786 Training loss: 4.1018 0.0342 sec/batch\n",
      "Epoch 1/1  Iteration 91/1786 Training loss: 4.0975 0.0430 sec/batch\n",
      "Epoch 1/1  Iteration 92/1786 Training loss: 4.0931 0.0350 sec/batch\n",
      "Epoch 1/1  Iteration 93/1786 Training loss: 4.0889 0.0345 sec/batch\n",
      "Epoch 1/1  Iteration 94/1786 Training loss: 4.0848 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 95/1786 Training loss: 4.0806 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 96/1786 Training loss: 4.0763 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 97/1786 Training loss: 4.0726 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 98/1786 Training loss: 4.0687 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 99/1786 Training loss: 4.0647 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 100/1786 Training loss: 4.0613 0.0357 sec/batch\n",
      "Validation loss: 3.48201 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 101/1786 Training loss: 4.0572 0.0236 sec/batch\n",
      "Epoch 1/1  Iteration 102/1786 Training loss: 4.0534 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 103/1786 Training loss: 4.0498 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 104/1786 Training loss: 4.0459 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 105/1786 Training loss: 4.0425 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 106/1786 Training loss: 4.0393 0.0350 sec/batch\n",
      "Epoch 1/1  Iteration 107/1786 Training loss: 4.0358 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 108/1786 Training loss: 4.0323 0.0335 sec/batch\n",
      "Epoch 1/1  Iteration 109/1786 Training loss: 4.0293 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 110/1786 Training loss: 4.0259 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 111/1786 Training loss: 4.0225 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 112/1786 Training loss: 4.0183 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 113/1786 Training loss: 4.0149 0.0374 sec/batch\n",
      "Epoch 1/1  Iteration 114/1786 Training loss: 4.0114 0.0349 sec/batch\n",
      "Epoch 1/1  Iteration 115/1786 Training loss: 4.0086 0.0426 sec/batch\n",
      "Epoch 1/1  Iteration 116/1786 Training loss: 4.0053 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 117/1786 Training loss: 4.0021 0.0376 sec/batch\n",
      "Epoch 1/1  Iteration 118/1786 Training loss: 3.9994 0.0356 sec/batch\n",
      "Epoch 1/1  Iteration 119/1786 Training loss: 3.9961 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 120/1786 Training loss: 3.9933 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 121/1786 Training loss: 3.9901 0.0362 sec/batch\n",
      "Epoch 1/1  Iteration 122/1786 Training loss: 3.9871 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 123/1786 Training loss: 3.9841 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 124/1786 Training loss: 3.9811 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 125/1786 Training loss: 3.9784 0.0250 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 126/1786 Training loss: 3.9756 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 127/1786 Training loss: 3.9727 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 128/1786 Training loss: 3.9698 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 129/1786 Training loss: 3.9665 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 130/1786 Training loss: 3.9632 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 131/1786 Training loss: 3.9599 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 132/1786 Training loss: 3.9566 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 133/1786 Training loss: 3.9537 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 134/1786 Training loss: 3.9507 0.0360 sec/batch\n",
      "Epoch 1/1  Iteration 135/1786 Training loss: 3.9479 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 136/1786 Training loss: 3.9452 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 137/1786 Training loss: 3.9428 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 138/1786 Training loss: 3.9399 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 139/1786 Training loss: 3.9371 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 140/1786 Training loss: 3.9342 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 141/1786 Training loss: 3.9315 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 142/1786 Training loss: 3.9287 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 143/1786 Training loss: 3.9262 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 144/1786 Training loss: 3.9236 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 145/1786 Training loss: 3.9212 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 146/1786 Training loss: 3.9183 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 147/1786 Training loss: 3.9157 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 148/1786 Training loss: 3.9128 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 149/1786 Training loss: 3.9099 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 150/1786 Training loss: 3.9075 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 151/1786 Training loss: 3.9049 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 152/1786 Training loss: 3.9023 0.0210 sec/batch\n",
      "Epoch 1/1  Iteration 153/1786 Training loss: 3.8995 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 154/1786 Training loss: 3.8973 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 155/1786 Training loss: 3.8949 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 156/1786 Training loss: 3.8925 0.0328 sec/batch\n",
      "Epoch 1/1  Iteration 157/1786 Training loss: 3.8904 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 158/1786 Training loss: 3.8878 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 159/1786 Training loss: 3.8854 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 160/1786 Training loss: 3.8829 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 161/1786 Training loss: 3.8805 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 162/1786 Training loss: 3.8778 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 163/1786 Training loss: 3.8751 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 164/1786 Training loss: 3.8728 0.0324 sec/batch\n",
      "Epoch 1/1  Iteration 165/1786 Training loss: 3.8705 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 166/1786 Training loss: 3.8678 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 167/1786 Training loss: 3.8655 0.0213 sec/batch\n",
      "Epoch 1/1  Iteration 168/1786 Training loss: 3.8633 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 169/1786 Training loss: 3.8615 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 170/1786 Training loss: 3.8597 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 171/1786 Training loss: 3.8574 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 172/1786 Training loss: 3.8549 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 173/1786 Training loss: 3.8525 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 174/1786 Training loss: 3.8501 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 175/1786 Training loss: 3.8479 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 176/1786 Training loss: 3.8458 0.0338 sec/batch\n",
      "Epoch 1/1  Iteration 177/1786 Training loss: 3.8436 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 178/1786 Training loss: 3.8416 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 179/1786 Training loss: 3.8396 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 180/1786 Training loss: 3.8374 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 181/1786 Training loss: 3.8356 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 182/1786 Training loss: 3.8335 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 183/1786 Training loss: 3.8318 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 184/1786 Training loss: 3.8297 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 185/1786 Training loss: 3.8275 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 186/1786 Training loss: 3.8253 0.0211 sec/batch\n",
      "Epoch 1/1  Iteration 187/1786 Training loss: 3.8233 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 188/1786 Training loss: 3.8211 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 189/1786 Training loss: 3.8190 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 190/1786 Training loss: 3.8170 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 191/1786 Training loss: 3.8149 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 192/1786 Training loss: 3.8129 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 193/1786 Training loss: 3.8108 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 194/1786 Training loss: 3.8088 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 195/1786 Training loss: 3.8068 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 196/1786 Training loss: 3.8050 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 197/1786 Training loss: 3.8029 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 198/1786 Training loss: 3.8011 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 199/1786 Training loss: 3.7992 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 200/1786 Training loss: 3.7974 0.0336 sec/batch\n",
      "Validation loss: 3.24893 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 201/1786 Training loss: 3.7956 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 202/1786 Training loss: 3.7939 0.0203 sec/batch\n",
      "Epoch 1/1  Iteration 203/1786 Training loss: 3.7924 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 204/1786 Training loss: 3.7903 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 205/1786 Training loss: 3.7884 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 206/1786 Training loss: 3.7865 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 207/1786 Training loss: 3.7848 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 208/1786 Training loss: 3.7831 0.0340 sec/batch\n",
      "Epoch 1/1  Iteration 209/1786 Training loss: 3.7813 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 210/1786 Training loss: 3.7795 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 211/1786 Training loss: 3.7777 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 212/1786 Training loss: 3.7760 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 213/1786 Training loss: 3.7744 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 214/1786 Training loss: 3.7727 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 215/1786 Training loss: 3.7709 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 216/1786 Training loss: 3.7692 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 217/1786 Training loss: 3.7673 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 218/1786 Training loss: 3.7656 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 219/1786 Training loss: 3.7641 0.0236 sec/batch\n",
      "Epoch 1/1  Iteration 220/1786 Training loss: 3.7627 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 221/1786 Training loss: 3.7611 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 222/1786 Training loss: 3.7595 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 223/1786 Training loss: 3.7578 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 224/1786 Training loss: 3.7562 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 225/1786 Training loss: 3.7546 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 226/1786 Training loss: 3.7530 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 227/1786 Training loss: 3.7515 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 228/1786 Training loss: 3.7499 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 229/1786 Training loss: 3.7483 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 230/1786 Training loss: 3.7468 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 231/1786 Training loss: 3.7452 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 232/1786 Training loss: 3.7436 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 233/1786 Training loss: 3.7422 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 234/1786 Training loss: 3.7404 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 235/1786 Training loss: 3.7389 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 236/1786 Training loss: 3.7373 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 237/1786 Training loss: 3.7357 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 238/1786 Training loss: 3.7339 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 239/1786 Training loss: 3.7324 0.0195 sec/batch\n",
      "Epoch 1/1  Iteration 240/1786 Training loss: 3.7313 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 241/1786 Training loss: 3.7299 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 242/1786 Training loss: 3.7285 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 243/1786 Training loss: 3.7266 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 244/1786 Training loss: 3.7249 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 245/1786 Training loss: 3.7235 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 246/1786 Training loss: 3.7222 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 247/1786 Training loss: 3.7208 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 248/1786 Training loss: 3.7192 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 249/1786 Training loss: 3.7177 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 250/1786 Training loss: 3.7159 0.0302 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 251/1786 Training loss: 3.7146 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 252/1786 Training loss: 3.7129 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 253/1786 Training loss: 3.7116 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 254/1786 Training loss: 3.7100 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 255/1786 Training loss: 3.7085 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 256/1786 Training loss: 3.7071 0.0201 sec/batch\n",
      "Epoch 1/1  Iteration 257/1786 Training loss: 3.7057 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 258/1786 Training loss: 3.7043 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 259/1786 Training loss: 3.7031 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 260/1786 Training loss: 3.7016 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 261/1786 Training loss: 3.7003 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 262/1786 Training loss: 3.6993 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 263/1786 Training loss: 3.6978 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 264/1786 Training loss: 3.6965 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 265/1786 Training loss: 3.6950 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 266/1786 Training loss: 3.6938 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 267/1786 Training loss: 3.6926 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 268/1786 Training loss: 3.6913 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 269/1786 Training loss: 3.6900 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 270/1786 Training loss: 3.6887 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 271/1786 Training loss: 3.6875 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 272/1786 Training loss: 3.6863 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 273/1786 Training loss: 3.6849 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 274/1786 Training loss: 3.6837 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 275/1786 Training loss: 3.6824 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 276/1786 Training loss: 3.6811 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 277/1786 Training loss: 3.6800 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 278/1786 Training loss: 3.6786 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 279/1786 Training loss: 3.6774 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 280/1786 Training loss: 3.6761 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 281/1786 Training loss: 3.6749 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 282/1786 Training loss: 3.6736 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 283/1786 Training loss: 3.6723 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 284/1786 Training loss: 3.6711 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 285/1786 Training loss: 3.6700 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 286/1786 Training loss: 3.6688 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 287/1786 Training loss: 3.6677 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 288/1786 Training loss: 3.6664 0.0211 sec/batch\n",
      "Epoch 1/1  Iteration 289/1786 Training loss: 3.6652 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 290/1786 Training loss: 3.6640 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 291/1786 Training loss: 3.6626 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 292/1786 Training loss: 3.6614 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 293/1786 Training loss: 3.6601 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 294/1786 Training loss: 3.6590 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 295/1786 Training loss: 3.6577 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 296/1786 Training loss: 3.6565 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 297/1786 Training loss: 3.6552 0.0339 sec/batch\n",
      "Epoch 1/1  Iteration 298/1786 Training loss: 3.6538 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 299/1786 Training loss: 3.6526 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 300/1786 Training loss: 3.6514 0.0234 sec/batch\n",
      "Validation loss: 3.1642 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 301/1786 Training loss: 3.6504 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 302/1786 Training loss: 3.6492 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 303/1786 Training loss: 3.6481 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 304/1786 Training loss: 3.6468 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 305/1786 Training loss: 3.6457 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 306/1786 Training loss: 3.6445 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 307/1786 Training loss: 3.6437 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 308/1786 Training loss: 3.6425 0.0350 sec/batch\n",
      "Epoch 1/1  Iteration 309/1786 Training loss: 3.6413 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 310/1786 Training loss: 3.6403 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 311/1786 Training loss: 3.6394 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 312/1786 Training loss: 3.6383 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 313/1786 Training loss: 3.6371 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 314/1786 Training loss: 3.6361 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 315/1786 Training loss: 3.6352 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 316/1786 Training loss: 3.6342 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 317/1786 Training loss: 3.6330 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 318/1786 Training loss: 3.6318 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 319/1786 Training loss: 3.6308 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 320/1786 Training loss: 3.6298 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 321/1786 Training loss: 3.6287 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 322/1786 Training loss: 3.6275 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 323/1786 Training loss: 3.6266 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 324/1786 Training loss: 3.6255 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 325/1786 Training loss: 3.6244 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 326/1786 Training loss: 3.6233 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 327/1786 Training loss: 3.6221 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 328/1786 Training loss: 3.6211 0.0324 sec/batch\n",
      "Epoch 1/1  Iteration 329/1786 Training loss: 3.6201 0.0322 sec/batch\n",
      "Epoch 1/1  Iteration 330/1786 Training loss: 3.6191 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 331/1786 Training loss: 3.6180 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 332/1786 Training loss: 3.6169 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 333/1786 Training loss: 3.6158 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 334/1786 Training loss: 3.6150 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 335/1786 Training loss: 3.6139 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 336/1786 Training loss: 3.6131 0.0337 sec/batch\n",
      "Epoch 1/1  Iteration 337/1786 Training loss: 3.6120 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 338/1786 Training loss: 3.6111 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 339/1786 Training loss: 3.6101 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 340/1786 Training loss: 3.6091 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 341/1786 Training loss: 3.6080 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 342/1786 Training loss: 3.6069 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 343/1786 Training loss: 3.6059 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 344/1786 Training loss: 3.6051 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 345/1786 Training loss: 3.6041 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 346/1786 Training loss: 3.6031 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 347/1786 Training loss: 3.6023 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 348/1786 Training loss: 3.6013 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 349/1786 Training loss: 3.6001 0.0223 sec/batch\n",
      "Epoch 1/1  Iteration 350/1786 Training loss: 3.5991 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 351/1786 Training loss: 3.5981 0.0337 sec/batch\n",
      "Epoch 1/1  Iteration 352/1786 Training loss: 3.5970 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 353/1786 Training loss: 3.5961 0.0209 sec/batch\n",
      "Epoch 1/1  Iteration 354/1786 Training loss: 3.5951 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 355/1786 Training loss: 3.5941 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 356/1786 Training loss: 3.5932 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 357/1786 Training loss: 3.5922 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 358/1786 Training loss: 3.5912 0.0361 sec/batch\n",
      "Epoch 1/1  Iteration 359/1786 Training loss: 3.5904 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 360/1786 Training loss: 3.5896 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 361/1786 Training loss: 3.5886 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 362/1786 Training loss: 3.5877 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 363/1786 Training loss: 3.5868 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 364/1786 Training loss: 3.5860 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 365/1786 Training loss: 3.5850 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 366/1786 Training loss: 3.5841 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 367/1786 Training loss: 3.5829 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 368/1786 Training loss: 3.5821 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 369/1786 Training loss: 3.5813 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 370/1786 Training loss: 3.5803 0.0247 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 371/1786 Training loss: 3.5795 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 372/1786 Training loss: 3.5786 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 373/1786 Training loss: 3.5778 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 374/1786 Training loss: 3.5769 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 375/1786 Training loss: 3.5760 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 376/1786 Training loss: 3.5752 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 377/1786 Training loss: 3.5743 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 378/1786 Training loss: 3.5735 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 379/1786 Training loss: 3.5726 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 380/1786 Training loss: 3.5717 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 381/1786 Training loss: 3.5709 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 382/1786 Training loss: 3.5702 0.0211 sec/batch\n",
      "Epoch 1/1  Iteration 383/1786 Training loss: 3.5695 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 384/1786 Training loss: 3.5688 0.0193 sec/batch\n",
      "Epoch 1/1  Iteration 385/1786 Training loss: 3.5680 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 386/1786 Training loss: 3.5672 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 387/1786 Training loss: 3.5665 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 388/1786 Training loss: 3.5656 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 389/1786 Training loss: 3.5649 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 390/1786 Training loss: 3.5640 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 391/1786 Training loss: 3.5632 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 392/1786 Training loss: 3.5624 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 393/1786 Training loss: 3.5616 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 394/1786 Training loss: 3.5609 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 395/1786 Training loss: 3.5601 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 396/1786 Training loss: 3.5593 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 397/1786 Training loss: 3.5584 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 398/1786 Training loss: 3.5576 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 399/1786 Training loss: 3.5569 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 400/1786 Training loss: 3.5561 0.0249 sec/batch\n",
      "Validation loss: 3.12747 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 401/1786 Training loss: 3.5555 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 402/1786 Training loss: 3.5547 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 403/1786 Training loss: 3.5539 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 404/1786 Training loss: 3.5532 0.0350 sec/batch\n",
      "Epoch 1/1  Iteration 405/1786 Training loss: 3.5525 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 406/1786 Training loss: 3.5517 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 407/1786 Training loss: 3.5510 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 408/1786 Training loss: 3.5503 0.0358 sec/batch\n",
      "Epoch 1/1  Iteration 409/1786 Training loss: 3.5496 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 410/1786 Training loss: 3.5488 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 411/1786 Training loss: 3.5481 0.0352 sec/batch\n",
      "Epoch 1/1  Iteration 412/1786 Training loss: 3.5474 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 413/1786 Training loss: 3.5465 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 414/1786 Training loss: 3.5458 0.0395 sec/batch\n",
      "Epoch 1/1  Iteration 415/1786 Training loss: 3.5451 0.0402 sec/batch\n",
      "Epoch 1/1  Iteration 416/1786 Training loss: 3.5444 0.0494 sec/batch\n",
      "Epoch 1/1  Iteration 417/1786 Training loss: 3.5436 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 418/1786 Training loss: 3.5431 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 419/1786 Training loss: 3.5423 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 420/1786 Training loss: 3.5416 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 421/1786 Training loss: 3.5409 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 422/1786 Training loss: 3.5401 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 423/1786 Training loss: 3.5395 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 424/1786 Training loss: 3.5387 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 425/1786 Training loss: 3.5380 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 426/1786 Training loss: 3.5373 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 427/1786 Training loss: 3.5366 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 428/1786 Training loss: 3.5359 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 429/1786 Training loss: 3.5352 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 430/1786 Training loss: 3.5344 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 431/1786 Training loss: 3.5335 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 432/1786 Training loss: 3.5328 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 433/1786 Training loss: 3.5322 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 434/1786 Training loss: 3.5316 0.0209 sec/batch\n",
      "Epoch 1/1  Iteration 435/1786 Training loss: 3.5310 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 436/1786 Training loss: 3.5304 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 437/1786 Training loss: 3.5299 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 438/1786 Training loss: 3.5291 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 439/1786 Training loss: 3.5283 0.0218 sec/batch\n",
      "Epoch 1/1  Iteration 440/1786 Training loss: 3.5276 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 441/1786 Training loss: 3.5268 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 442/1786 Training loss: 3.5259 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 443/1786 Training loss: 3.5252 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 444/1786 Training loss: 3.5246 0.0215 sec/batch\n",
      "Epoch 1/1  Iteration 445/1786 Training loss: 3.5239 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 446/1786 Training loss: 3.5233 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 447/1786 Training loss: 3.5227 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 448/1786 Training loss: 3.5220 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 449/1786 Training loss: 3.5214 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 450/1786 Training loss: 3.5208 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 451/1786 Training loss: 3.5203 0.0341 sec/batch\n",
      "Epoch 1/1  Iteration 452/1786 Training loss: 3.5195 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 453/1786 Training loss: 3.5188 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 454/1786 Training loss: 3.5182 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 455/1786 Training loss: 3.5176 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 456/1786 Training loss: 3.5170 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 457/1786 Training loss: 3.5163 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 458/1786 Training loss: 3.5157 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 459/1786 Training loss: 3.5152 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 460/1786 Training loss: 3.5146 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 461/1786 Training loss: 3.5139 0.0205 sec/batch\n",
      "Epoch 1/1  Iteration 462/1786 Training loss: 3.5132 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 463/1786 Training loss: 3.5125 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 464/1786 Training loss: 3.5119 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 465/1786 Training loss: 3.5112 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 466/1786 Training loss: 3.5106 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 467/1786 Training loss: 3.5100 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 468/1786 Training loss: 3.5094 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 469/1786 Training loss: 3.5088 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 470/1786 Training loss: 3.5082 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 471/1786 Training loss: 3.5074 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 472/1786 Training loss: 3.5067 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 473/1786 Training loss: 3.5061 0.0329 sec/batch\n",
      "Epoch 1/1  Iteration 474/1786 Training loss: 3.5055 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 475/1786 Training loss: 3.5049 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 476/1786 Training loss: 3.5042 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 477/1786 Training loss: 3.5036 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 478/1786 Training loss: 3.5030 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 479/1786 Training loss: 3.5025 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 480/1786 Training loss: 3.5020 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 481/1786 Training loss: 3.5015 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 482/1786 Training loss: 3.5009 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 483/1786 Training loss: 3.5004 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 484/1786 Training loss: 3.4998 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 485/1786 Training loss: 3.4991 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 486/1786 Training loss: 3.4984 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 487/1786 Training loss: 3.4978 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 488/1786 Training loss: 3.4974 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 489/1786 Training loss: 3.4968 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 490/1786 Training loss: 3.4962 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 491/1786 Training loss: 3.4957 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 492/1786 Training loss: 3.4950 0.0259 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 493/1786 Training loss: 3.4944 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 494/1786 Training loss: 3.4937 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 495/1786 Training loss: 3.4931 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 496/1786 Training loss: 3.4925 0.0211 sec/batch\n",
      "Epoch 1/1  Iteration 497/1786 Training loss: 3.4918 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 498/1786 Training loss: 3.4912 0.0371 sec/batch\n",
      "Epoch 1/1  Iteration 499/1786 Training loss: 3.4907 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 500/1786 Training loss: 3.4902 0.0307 sec/batch\n",
      "Validation loss: 3.10919 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 501/1786 Training loss: 3.4896 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 502/1786 Training loss: 3.4891 0.0236 sec/batch\n",
      "Epoch 1/1  Iteration 503/1786 Training loss: 3.4884 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 504/1786 Training loss: 3.4879 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 505/1786 Training loss: 3.4873 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 506/1786 Training loss: 3.4866 0.0213 sec/batch\n",
      "Epoch 1/1  Iteration 507/1786 Training loss: 3.4861 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 508/1786 Training loss: 3.4855 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 509/1786 Training loss: 3.4849 0.0339 sec/batch\n",
      "Epoch 1/1  Iteration 510/1786 Training loss: 3.4843 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 511/1786 Training loss: 3.4837 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 512/1786 Training loss: 3.4832 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 513/1786 Training loss: 3.4827 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 514/1786 Training loss: 3.4821 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 515/1786 Training loss: 3.4816 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 516/1786 Training loss: 3.4810 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 517/1786 Training loss: 3.4805 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 518/1786 Training loss: 3.4799 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 519/1786 Training loss: 3.4793 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 520/1786 Training loss: 3.4788 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 521/1786 Training loss: 3.4781 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 522/1786 Training loss: 3.4776 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 523/1786 Training loss: 3.4770 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 524/1786 Training loss: 3.4767 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 525/1786 Training loss: 3.4761 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 526/1786 Training loss: 3.4757 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 527/1786 Training loss: 3.4751 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 528/1786 Training loss: 3.4745 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 529/1786 Training loss: 3.4740 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 530/1786 Training loss: 3.4735 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 531/1786 Training loss: 3.4730 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 532/1786 Training loss: 3.4725 0.0363 sec/batch\n",
      "Epoch 1/1  Iteration 533/1786 Training loss: 3.4720 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 534/1786 Training loss: 3.4714 0.0359 sec/batch\n",
      "Epoch 1/1  Iteration 535/1786 Training loss: 3.4708 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 536/1786 Training loss: 3.4703 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 537/1786 Training loss: 3.4698 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 538/1786 Training loss: 3.4693 0.0374 sec/batch\n",
      "Epoch 1/1  Iteration 539/1786 Training loss: 3.4688 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 540/1786 Training loss: 3.4683 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 541/1786 Training loss: 3.4679 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 542/1786 Training loss: 3.4674 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 543/1786 Training loss: 3.4669 0.0345 sec/batch\n",
      "Epoch 1/1  Iteration 544/1786 Training loss: 3.4663 0.0335 sec/batch\n",
      "Epoch 1/1  Iteration 545/1786 Training loss: 3.4659 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 546/1786 Training loss: 3.4654 0.0328 sec/batch\n",
      "Epoch 1/1  Iteration 547/1786 Training loss: 3.4648 0.0337 sec/batch\n",
      "Epoch 1/1  Iteration 548/1786 Training loss: 3.4643 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 549/1786 Training loss: 3.4638 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 550/1786 Training loss: 3.4633 0.0371 sec/batch\n",
      "Epoch 1/1  Iteration 551/1786 Training loss: 3.4628 0.0343 sec/batch\n",
      "Epoch 1/1  Iteration 552/1786 Training loss: 3.4623 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 553/1786 Training loss: 3.4618 0.0356 sec/batch\n",
      "Epoch 1/1  Iteration 554/1786 Training loss: 3.4613 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 555/1786 Training loss: 3.4608 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 556/1786 Training loss: 3.4604 0.0328 sec/batch\n",
      "Epoch 1/1  Iteration 557/1786 Training loss: 3.4599 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 558/1786 Training loss: 3.4595 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 559/1786 Training loss: 3.4590 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 560/1786 Training loss: 3.4585 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 561/1786 Training loss: 3.4579 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 562/1786 Training loss: 3.4575 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 563/1786 Training loss: 3.4570 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 564/1786 Training loss: 3.4567 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 565/1786 Training loss: 3.4562 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 566/1786 Training loss: 3.4559 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 567/1786 Training loss: 3.4554 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 568/1786 Training loss: 3.4549 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 569/1786 Training loss: 3.4545 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 570/1786 Training loss: 3.4541 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 571/1786 Training loss: 3.4536 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 572/1786 Training loss: 3.4533 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 573/1786 Training loss: 3.4527 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 574/1786 Training loss: 3.4523 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 575/1786 Training loss: 3.4519 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 576/1786 Training loss: 3.4515 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 577/1786 Training loss: 3.4511 0.0329 sec/batch\n",
      "Epoch 1/1  Iteration 578/1786 Training loss: 3.4507 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 579/1786 Training loss: 3.4503 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 580/1786 Training loss: 3.4498 0.0209 sec/batch\n",
      "Epoch 1/1  Iteration 581/1786 Training loss: 3.4494 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 582/1786 Training loss: 3.4490 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 583/1786 Training loss: 3.4485 0.0205 sec/batch\n",
      "Epoch 1/1  Iteration 584/1786 Training loss: 3.4481 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 585/1786 Training loss: 3.4476 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 586/1786 Training loss: 3.4472 0.0348 sec/batch\n",
      "Epoch 1/1  Iteration 587/1786 Training loss: 3.4469 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 588/1786 Training loss: 3.4464 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 589/1786 Training loss: 3.4460 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 590/1786 Training loss: 3.4455 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 591/1786 Training loss: 3.4451 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 592/1786 Training loss: 3.4448 0.0329 sec/batch\n",
      "Epoch 1/1  Iteration 593/1786 Training loss: 3.4443 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 594/1786 Training loss: 3.4438 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 595/1786 Training loss: 3.4434 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 596/1786 Training loss: 3.4429 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 597/1786 Training loss: 3.4424 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 598/1786 Training loss: 3.4419 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 599/1786 Training loss: 3.4414 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 600/1786 Training loss: 3.4410 0.0255 sec/batch\n",
      "Validation loss: 3.09174 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 601/1786 Training loss: 3.4407 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 602/1786 Training loss: 3.4402 0.0203 sec/batch\n",
      "Epoch 1/1  Iteration 603/1786 Training loss: 3.4398 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 604/1786 Training loss: 3.4393 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 605/1786 Training loss: 3.4388 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 606/1786 Training loss: 3.4383 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 607/1786 Training loss: 3.4378 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 608/1786 Training loss: 3.4373 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 609/1786 Training loss: 3.4368 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 610/1786 Training loss: 3.4364 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 611/1786 Training loss: 3.4359 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 612/1786 Training loss: 3.4354 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 613/1786 Training loss: 3.4351 0.0292 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 614/1786 Training loss: 3.4347 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 615/1786 Training loss: 3.4343 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 616/1786 Training loss: 3.4338 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 617/1786 Training loss: 3.4334 0.0201 sec/batch\n",
      "Epoch 1/1  Iteration 618/1786 Training loss: 3.4329 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 619/1786 Training loss: 3.4324 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 620/1786 Training loss: 3.4319 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 621/1786 Training loss: 3.4316 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 622/1786 Training loss: 3.4312 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 623/1786 Training loss: 3.4308 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 624/1786 Training loss: 3.4303 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 625/1786 Training loss: 3.4299 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 626/1786 Training loss: 3.4294 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 627/1786 Training loss: 3.4290 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 628/1786 Training loss: 3.4286 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 629/1786 Training loss: 3.4281 0.0330 sec/batch\n",
      "Epoch 1/1  Iteration 630/1786 Training loss: 3.4278 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 631/1786 Training loss: 3.4274 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 632/1786 Training loss: 3.4271 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 633/1786 Training loss: 3.4266 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 634/1786 Training loss: 3.4262 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 635/1786 Training loss: 3.4258 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 636/1786 Training loss: 3.4254 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 637/1786 Training loss: 3.4249 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 638/1786 Training loss: 3.4245 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 639/1786 Training loss: 3.4241 0.0215 sec/batch\n",
      "Epoch 1/1  Iteration 640/1786 Training loss: 3.4237 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 641/1786 Training loss: 3.4233 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 642/1786 Training loss: 3.4230 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 643/1786 Training loss: 3.4226 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 644/1786 Training loss: 3.4223 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 645/1786 Training loss: 3.4219 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 646/1786 Training loss: 3.4214 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 647/1786 Training loss: 3.4211 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 648/1786 Training loss: 3.4207 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 649/1786 Training loss: 3.4202 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 650/1786 Training loss: 3.4198 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 651/1786 Training loss: 3.4193 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 652/1786 Training loss: 3.4189 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 653/1786 Training loss: 3.4184 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 654/1786 Training loss: 3.4180 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 655/1786 Training loss: 3.4176 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 656/1786 Training loss: 3.4172 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 657/1786 Training loss: 3.4167 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 658/1786 Training loss: 3.4162 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 659/1786 Training loss: 3.4158 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 660/1786 Training loss: 3.4154 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 661/1786 Training loss: 3.4150 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 662/1786 Training loss: 3.4146 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 663/1786 Training loss: 3.4141 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 664/1786 Training loss: 3.4137 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 665/1786 Training loss: 3.4132 0.0339 sec/batch\n",
      "Epoch 1/1  Iteration 666/1786 Training loss: 3.4127 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 667/1786 Training loss: 3.4123 0.0199 sec/batch\n",
      "Epoch 1/1  Iteration 668/1786 Training loss: 3.4120 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 669/1786 Training loss: 3.4115 0.0200 sec/batch\n",
      "Epoch 1/1  Iteration 670/1786 Training loss: 3.4111 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 671/1786 Training loss: 3.4107 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 672/1786 Training loss: 3.4103 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 673/1786 Training loss: 3.4100 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 674/1786 Training loss: 3.4095 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 675/1786 Training loss: 3.4092 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 676/1786 Training loss: 3.4088 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 677/1786 Training loss: 3.4083 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 678/1786 Training loss: 3.4078 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 679/1786 Training loss: 3.4074 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 680/1786 Training loss: 3.4070 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 681/1786 Training loss: 3.4066 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 682/1786 Training loss: 3.4062 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 683/1786 Training loss: 3.4058 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 684/1786 Training loss: 3.4053 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 685/1786 Training loss: 3.4048 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 686/1786 Training loss: 3.4045 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 687/1786 Training loss: 3.4041 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 688/1786 Training loss: 3.4037 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 689/1786 Training loss: 3.4034 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 690/1786 Training loss: 3.4030 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 691/1786 Training loss: 3.4026 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 692/1786 Training loss: 3.4024 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 693/1786 Training loss: 3.4021 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 694/1786 Training loss: 3.4016 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 695/1786 Training loss: 3.4013 0.0348 sec/batch\n",
      "Epoch 1/1  Iteration 696/1786 Training loss: 3.4009 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 697/1786 Training loss: 3.4005 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 698/1786 Training loss: 3.4001 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 699/1786 Training loss: 3.3998 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 700/1786 Training loss: 3.3994 0.0273 sec/batch\n",
      "Validation loss: 3.07257 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 701/1786 Training loss: 3.3990 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 702/1786 Training loss: 3.3986 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 703/1786 Training loss: 3.3983 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 704/1786 Training loss: 3.3978 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 705/1786 Training loss: 3.3975 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 706/1786 Training loss: 3.3970 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 707/1786 Training loss: 3.3966 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 708/1786 Training loss: 3.3963 0.0356 sec/batch\n",
      "Epoch 1/1  Iteration 709/1786 Training loss: 3.3959 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 710/1786 Training loss: 3.3956 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 711/1786 Training loss: 3.3953 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 712/1786 Training loss: 3.3949 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 713/1786 Training loss: 3.3945 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 714/1786 Training loss: 3.3942 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 715/1786 Training loss: 3.3938 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 716/1786 Training loss: 3.3934 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 717/1786 Training loss: 3.3931 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 718/1786 Training loss: 3.3927 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 719/1786 Training loss: 3.3923 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 720/1786 Training loss: 3.3920 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 721/1786 Training loss: 3.3917 0.0463 sec/batch\n",
      "Epoch 1/1  Iteration 722/1786 Training loss: 3.3914 0.0475 sec/batch\n",
      "Epoch 1/1  Iteration 723/1786 Training loss: 3.3910 0.0452 sec/batch\n",
      "Epoch 1/1  Iteration 724/1786 Training loss: 3.3907 0.0545 sec/batch\n",
      "Epoch 1/1  Iteration 725/1786 Training loss: 3.3904 0.0340 sec/batch\n",
      "Epoch 1/1  Iteration 726/1786 Training loss: 3.3901 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 727/1786 Training loss: 3.3897 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 728/1786 Training loss: 3.3893 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 729/1786 Training loss: 3.3890 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 730/1786 Training loss: 3.3885 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 731/1786 Training loss: 3.3882 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 732/1786 Training loss: 3.3879 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 733/1786 Training loss: 3.3875 0.0311 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 734/1786 Training loss: 3.3871 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 735/1786 Training loss: 3.3868 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 736/1786 Training loss: 3.3864 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 737/1786 Training loss: 3.3860 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 738/1786 Training loss: 3.3857 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 739/1786 Training loss: 3.3853 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 740/1786 Training loss: 3.3850 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 741/1786 Training loss: 3.3847 0.0381 sec/batch\n",
      "Epoch 1/1  Iteration 742/1786 Training loss: 3.3843 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 743/1786 Training loss: 3.3840 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 744/1786 Training loss: 3.3836 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 745/1786 Training loss: 3.3833 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 746/1786 Training loss: 3.3829 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 747/1786 Training loss: 3.3826 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 748/1786 Training loss: 3.3823 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 749/1786 Training loss: 3.3819 0.0213 sec/batch\n",
      "Epoch 1/1  Iteration 750/1786 Training loss: 3.3817 0.0210 sec/batch\n",
      "Epoch 1/1  Iteration 751/1786 Training loss: 3.3813 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 752/1786 Training loss: 3.3810 0.0339 sec/batch\n",
      "Epoch 1/1  Iteration 753/1786 Training loss: 3.3805 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 754/1786 Training loss: 3.3802 0.0408 sec/batch\n",
      "Epoch 1/1  Iteration 755/1786 Training loss: 3.3800 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 756/1786 Training loss: 3.3795 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 757/1786 Training loss: 3.3792 0.0326 sec/batch\n",
      "Epoch 1/1  Iteration 758/1786 Training loss: 3.3789 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 759/1786 Training loss: 3.3787 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 760/1786 Training loss: 3.3785 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 761/1786 Training loss: 3.3781 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 762/1786 Training loss: 3.3777 0.0322 sec/batch\n",
      "Epoch 1/1  Iteration 763/1786 Training loss: 3.3773 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 764/1786 Training loss: 3.3770 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 765/1786 Training loss: 3.3767 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 766/1786 Training loss: 3.3763 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 767/1786 Training loss: 3.3760 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 768/1786 Training loss: 3.3756 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 769/1786 Training loss: 3.3753 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 770/1786 Training loss: 3.3750 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 771/1786 Training loss: 3.3747 0.0831 sec/batch\n",
      "Epoch 1/1  Iteration 772/1786 Training loss: 3.3744 0.0476 sec/batch\n",
      "Epoch 1/1  Iteration 773/1786 Training loss: 3.3741 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 774/1786 Training loss: 3.3738 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 775/1786 Training loss: 3.3735 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 776/1786 Training loss: 3.3732 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 777/1786 Training loss: 3.3728 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 778/1786 Training loss: 3.3725 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 779/1786 Training loss: 3.3722 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 780/1786 Training loss: 3.3719 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 781/1786 Training loss: 3.3716 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 782/1786 Training loss: 3.3713 0.0215 sec/batch\n",
      "Epoch 1/1  Iteration 783/1786 Training loss: 3.3709 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 784/1786 Training loss: 3.3705 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 785/1786 Training loss: 3.3703 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 786/1786 Training loss: 3.3700 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 787/1786 Training loss: 3.3696 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 788/1786 Training loss: 3.3693 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 789/1786 Training loss: 3.3690 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 790/1786 Training loss: 3.3688 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 791/1786 Training loss: 3.3685 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 792/1786 Training loss: 3.3682 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 793/1786 Training loss: 3.3679 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 794/1786 Training loss: 3.3677 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 795/1786 Training loss: 3.3674 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 796/1786 Training loss: 3.3670 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 797/1786 Training loss: 3.3667 0.0344 sec/batch\n",
      "Epoch 1/1  Iteration 798/1786 Training loss: 3.3665 0.0337 sec/batch\n",
      "Epoch 1/1  Iteration 799/1786 Training loss: 3.3662 0.0213 sec/batch\n",
      "Epoch 1/1  Iteration 800/1786 Training loss: 3.3659 0.0268 sec/batch\n",
      "Validation loss: 3.05412 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 801/1786 Training loss: 3.3657 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 802/1786 Training loss: 3.3653 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 803/1786 Training loss: 3.3650 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 804/1786 Training loss: 3.3647 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 805/1786 Training loss: 3.3644 0.0351 sec/batch\n",
      "Epoch 1/1  Iteration 806/1786 Training loss: 3.3641 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 807/1786 Training loss: 3.3639 0.0383 sec/batch\n",
      "Epoch 1/1  Iteration 808/1786 Training loss: 3.3635 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 809/1786 Training loss: 3.3632 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 810/1786 Training loss: 3.3629 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 811/1786 Training loss: 3.3625 0.0192 sec/batch\n",
      "Epoch 1/1  Iteration 812/1786 Training loss: 3.3623 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 813/1786 Training loss: 3.3620 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 814/1786 Training loss: 3.3618 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 815/1786 Training loss: 3.3615 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 816/1786 Training loss: 3.3612 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 817/1786 Training loss: 3.3609 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 818/1786 Training loss: 3.3606 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 819/1786 Training loss: 3.3602 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 820/1786 Training loss: 3.3599 0.0354 sec/batch\n",
      "Epoch 1/1  Iteration 821/1786 Training loss: 3.3595 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 822/1786 Training loss: 3.3592 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 823/1786 Training loss: 3.3590 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 824/1786 Training loss: 3.3587 0.0346 sec/batch\n",
      "Epoch 1/1  Iteration 825/1786 Training loss: 3.3585 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 826/1786 Training loss: 3.3582 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 827/1786 Training loss: 3.3578 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 828/1786 Training loss: 3.3575 0.0374 sec/batch\n",
      "Epoch 1/1  Iteration 829/1786 Training loss: 3.3573 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 830/1786 Training loss: 3.3570 0.0362 sec/batch\n",
      "Epoch 1/1  Iteration 831/1786 Training loss: 3.3567 0.0356 sec/batch\n",
      "Epoch 1/1  Iteration 832/1786 Training loss: 3.3564 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 833/1786 Training loss: 3.3561 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 834/1786 Training loss: 3.3558 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 835/1786 Training loss: 3.3556 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 836/1786 Training loss: 3.3553 0.0394 sec/batch\n",
      "Epoch 1/1  Iteration 837/1786 Training loss: 3.3550 0.0336 sec/batch\n",
      "Epoch 1/1  Iteration 838/1786 Training loss: 3.3548 0.0350 sec/batch\n",
      "Epoch 1/1  Iteration 839/1786 Training loss: 3.3546 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 840/1786 Training loss: 3.3544 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 841/1786 Training loss: 3.3541 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 842/1786 Training loss: 3.3539 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 843/1786 Training loss: 3.3536 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 844/1786 Training loss: 3.3533 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 845/1786 Training loss: 3.3530 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 846/1786 Training loss: 3.3526 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 847/1786 Training loss: 3.3523 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 848/1786 Training loss: 3.3520 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 849/1786 Training loss: 3.3518 0.0406 sec/batch\n",
      "Epoch 1/1  Iteration 850/1786 Training loss: 3.3515 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 851/1786 Training loss: 3.3513 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 852/1786 Training loss: 3.3510 0.0279 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 853/1786 Training loss: 3.3508 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 854/1786 Training loss: 3.3506 0.0361 sec/batch\n",
      "Epoch 1/1  Iteration 855/1786 Training loss: 3.3503 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 856/1786 Training loss: 3.3500 0.0417 sec/batch\n",
      "Epoch 1/1  Iteration 857/1786 Training loss: 3.3497 0.0417 sec/batch\n",
      "Epoch 1/1  Iteration 858/1786 Training loss: 3.3495 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 859/1786 Training loss: 3.3493 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 860/1786 Training loss: 3.3490 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 861/1786 Training loss: 3.3488 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 862/1786 Training loss: 3.3486 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 863/1786 Training loss: 3.3483 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 864/1786 Training loss: 3.3481 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 865/1786 Training loss: 3.3478 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 866/1786 Training loss: 3.3475 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 867/1786 Training loss: 3.3473 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 868/1786 Training loss: 3.3470 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 869/1786 Training loss: 3.3467 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 870/1786 Training loss: 3.3465 0.0339 sec/batch\n",
      "Epoch 1/1  Iteration 871/1786 Training loss: 3.3462 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 872/1786 Training loss: 3.3460 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 873/1786 Training loss: 3.3457 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 874/1786 Training loss: 3.3455 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 875/1786 Training loss: 3.3453 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 876/1786 Training loss: 3.3450 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 877/1786 Training loss: 3.3448 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 878/1786 Training loss: 3.3446 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 879/1786 Training loss: 3.3443 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 880/1786 Training loss: 3.3439 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 881/1786 Training loss: 3.3437 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 882/1786 Training loss: 3.3434 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 883/1786 Training loss: 3.3432 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 884/1786 Training loss: 3.3429 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 885/1786 Training loss: 3.3427 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 886/1786 Training loss: 3.3424 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 887/1786 Training loss: 3.3422 0.0205 sec/batch\n",
      "Epoch 1/1  Iteration 888/1786 Training loss: 3.3419 0.0197 sec/batch\n",
      "Epoch 1/1  Iteration 889/1786 Training loss: 3.3417 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 890/1786 Training loss: 3.3414 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 891/1786 Training loss: 3.3411 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 892/1786 Training loss: 3.3409 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 893/1786 Training loss: 3.3407 0.0343 sec/batch\n",
      "Epoch 1/1  Iteration 894/1786 Training loss: 3.3405 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 895/1786 Training loss: 3.3401 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 896/1786 Training loss: 3.3399 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 897/1786 Training loss: 3.3397 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 898/1786 Training loss: 3.3394 0.0352 sec/batch\n",
      "Epoch 1/1  Iteration 899/1786 Training loss: 3.3392 0.0338 sec/batch\n",
      "Epoch 1/1  Iteration 900/1786 Training loss: 3.3390 0.0300 sec/batch\n",
      "Validation loss: 3.03674 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 901/1786 Training loss: 3.3387 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 902/1786 Training loss: 3.3385 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 903/1786 Training loss: 3.3382 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 904/1786 Training loss: 3.3379 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 905/1786 Training loss: 3.3377 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 906/1786 Training loss: 3.3374 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 907/1786 Training loss: 3.3373 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 908/1786 Training loss: 3.3370 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 909/1786 Training loss: 3.3368 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 910/1786 Training loss: 3.3366 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 911/1786 Training loss: 3.3364 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 912/1786 Training loss: 3.3362 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 913/1786 Training loss: 3.3359 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 914/1786 Training loss: 3.3357 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 915/1786 Training loss: 3.3354 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 916/1786 Training loss: 3.3351 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 917/1786 Training loss: 3.3349 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 918/1786 Training loss: 3.3347 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 919/1786 Training loss: 3.3344 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 920/1786 Training loss: 3.3342 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 921/1786 Training loss: 3.3340 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 922/1786 Training loss: 3.3337 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 923/1786 Training loss: 3.3335 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 924/1786 Training loss: 3.3332 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 925/1786 Training loss: 3.3331 0.0205 sec/batch\n",
      "Epoch 1/1  Iteration 926/1786 Training loss: 3.3328 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 927/1786 Training loss: 3.3326 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 928/1786 Training loss: 3.3323 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 929/1786 Training loss: 3.3320 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 930/1786 Training loss: 3.3318 0.0466 sec/batch\n",
      "Epoch 1/1  Iteration 931/1786 Training loss: 3.3316 0.0453 sec/batch\n",
      "Epoch 1/1  Iteration 932/1786 Training loss: 3.3313 0.0352 sec/batch\n",
      "Epoch 1/1  Iteration 933/1786 Training loss: 3.3311 0.0387 sec/batch\n",
      "Epoch 1/1  Iteration 934/1786 Training loss: 3.3308 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 935/1786 Training loss: 3.3306 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 936/1786 Training loss: 3.3304 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 937/1786 Training loss: 3.3302 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 938/1786 Training loss: 3.3300 0.0210 sec/batch\n",
      "Epoch 1/1  Iteration 939/1786 Training loss: 3.3297 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 940/1786 Training loss: 3.3295 0.0216 sec/batch\n",
      "Epoch 1/1  Iteration 941/1786 Training loss: 3.3292 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 942/1786 Training loss: 3.3289 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 943/1786 Training loss: 3.3287 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 944/1786 Training loss: 3.3284 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 945/1786 Training loss: 3.3282 0.0641 sec/batch\n",
      "Epoch 1/1  Iteration 946/1786 Training loss: 3.3280 0.0346 sec/batch\n",
      "Epoch 1/1  Iteration 947/1786 Training loss: 3.3277 0.0344 sec/batch\n",
      "Epoch 1/1  Iteration 948/1786 Training loss: 3.3274 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 949/1786 Training loss: 3.3271 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 950/1786 Training loss: 3.3268 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 951/1786 Training loss: 3.3266 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 952/1786 Training loss: 3.3263 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 953/1786 Training loss: 3.3261 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 954/1786 Training loss: 3.3258 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 955/1786 Training loss: 3.3256 0.0445 sec/batch\n",
      "Epoch 1/1  Iteration 956/1786 Training loss: 3.3254 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 957/1786 Training loss: 3.3252 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 958/1786 Training loss: 3.3249 0.0473 sec/batch\n",
      "Epoch 1/1  Iteration 959/1786 Training loss: 3.3247 0.0423 sec/batch\n",
      "Epoch 1/1  Iteration 960/1786 Training loss: 3.3245 0.0457 sec/batch\n",
      "Epoch 1/1  Iteration 961/1786 Training loss: 3.3242 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 962/1786 Training loss: 3.3240 0.0378 sec/batch\n",
      "Epoch 1/1  Iteration 963/1786 Training loss: 3.3238 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 964/1786 Training loss: 3.3236 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 965/1786 Training loss: 3.3234 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 966/1786 Training loss: 3.3232 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 967/1786 Training loss: 3.3230 0.0532 sec/batch\n",
      "Epoch 1/1  Iteration 968/1786 Training loss: 3.3228 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 969/1786 Training loss: 3.3226 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 970/1786 Training loss: 3.3224 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 971/1786 Training loss: 3.3222 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 972/1786 Training loss: 3.3220 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 973/1786 Training loss: 3.3217 0.0433 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 974/1786 Training loss: 3.3215 0.0431 sec/batch\n",
      "Epoch 1/1  Iteration 975/1786 Training loss: 3.3213 0.0485 sec/batch\n",
      "Epoch 1/1  Iteration 976/1786 Training loss: 3.3211 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 977/1786 Training loss: 3.3209 0.0360 sec/batch\n",
      "Epoch 1/1  Iteration 978/1786 Training loss: 3.3207 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 979/1786 Training loss: 3.3204 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 980/1786 Training loss: 3.3202 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 981/1786 Training loss: 3.3199 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 982/1786 Training loss: 3.3198 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 983/1786 Training loss: 3.3196 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 984/1786 Training loss: 3.3194 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 985/1786 Training loss: 3.3191 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 986/1786 Training loss: 3.3189 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 987/1786 Training loss: 3.3187 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 988/1786 Training loss: 3.3185 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 989/1786 Training loss: 3.3183 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 990/1786 Training loss: 3.3181 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 991/1786 Training loss: 3.3178 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 992/1786 Training loss: 3.3176 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 993/1786 Training loss: 3.3174 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 994/1786 Training loss: 3.3172 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 995/1786 Training loss: 3.3170 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 996/1786 Training loss: 3.3168 0.0216 sec/batch\n",
      "Epoch 1/1  Iteration 997/1786 Training loss: 3.3166 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 998/1786 Training loss: 3.3164 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 999/1786 Training loss: 3.3162 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1000/1786 Training loss: 3.3160 0.0373 sec/batch\n",
      "Validation loss: 3.02054 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1001/1786 Training loss: 3.3157 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1002/1786 Training loss: 3.3156 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1003/1786 Training loss: 3.3153 0.0217 sec/batch\n",
      "Epoch 1/1  Iteration 1004/1786 Training loss: 3.3151 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1005/1786 Training loss: 3.3149 0.0204 sec/batch\n",
      "Epoch 1/1  Iteration 1006/1786 Training loss: 3.3146 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 1007/1786 Training loss: 3.3145 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 1008/1786 Training loss: 3.3143 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 1009/1786 Training loss: 3.3141 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1010/1786 Training loss: 3.3139 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1011/1786 Training loss: 3.3137 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 1012/1786 Training loss: 3.3135 0.0358 sec/batch\n",
      "Epoch 1/1  Iteration 1013/1786 Training loss: 3.3133 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 1014/1786 Training loss: 3.3131 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1015/1786 Training loss: 3.3129 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 1016/1786 Training loss: 3.3127 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 1017/1786 Training loss: 3.3125 0.0372 sec/batch\n",
      "Epoch 1/1  Iteration 1018/1786 Training loss: 3.3123 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1019/1786 Training loss: 3.3121 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1020/1786 Training loss: 3.3119 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1021/1786 Training loss: 3.3117 0.0205 sec/batch\n",
      "Epoch 1/1  Iteration 1022/1786 Training loss: 3.3115 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 1023/1786 Training loss: 3.3112 0.0213 sec/batch\n",
      "Epoch 1/1  Iteration 1024/1786 Training loss: 3.3109 0.0198 sec/batch\n",
      "Epoch 1/1  Iteration 1025/1786 Training loss: 3.3107 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 1026/1786 Training loss: 3.3105 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 1027/1786 Training loss: 3.3103 0.0210 sec/batch\n",
      "Epoch 1/1  Iteration 1028/1786 Training loss: 3.3101 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1029/1786 Training loss: 3.3099 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1030/1786 Training loss: 3.3097 0.0354 sec/batch\n",
      "Epoch 1/1  Iteration 1031/1786 Training loss: 3.3095 0.0626 sec/batch\n",
      "Epoch 1/1  Iteration 1032/1786 Training loss: 3.3093 0.0445 sec/batch\n",
      "Epoch 1/1  Iteration 1033/1786 Training loss: 3.3091 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1034/1786 Training loss: 3.3089 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 1035/1786 Training loss: 3.3087 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 1036/1786 Training loss: 3.3085 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1037/1786 Training loss: 3.3082 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 1038/1786 Training loss: 3.3080 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1039/1786 Training loss: 3.3078 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 1040/1786 Training loss: 3.3077 0.0328 sec/batch\n",
      "Epoch 1/1  Iteration 1041/1786 Training loss: 3.3074 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 1042/1786 Training loss: 3.3073 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1043/1786 Training loss: 3.3071 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 1044/1786 Training loss: 3.3069 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1045/1786 Training loss: 3.3066 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1046/1786 Training loss: 3.3064 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 1047/1786 Training loss: 3.3062 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 1048/1786 Training loss: 3.3060 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 1049/1786 Training loss: 3.3057 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1050/1786 Training loss: 3.3055 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1051/1786 Training loss: 3.3053 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1052/1786 Training loss: 3.3051 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1053/1786 Training loss: 3.3050 0.0502 sec/batch\n",
      "Epoch 1/1  Iteration 1054/1786 Training loss: 3.3048 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1055/1786 Training loss: 3.3046 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 1056/1786 Training loss: 3.3044 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 1057/1786 Training loss: 3.3042 0.0201 sec/batch\n",
      "Epoch 1/1  Iteration 1058/1786 Training loss: 3.3040 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1059/1786 Training loss: 3.3038 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1060/1786 Training loss: 3.3036 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 1061/1786 Training loss: 3.3034 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 1062/1786 Training loss: 3.3032 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 1063/1786 Training loss: 3.3030 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1064/1786 Training loss: 3.3028 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 1065/1786 Training loss: 3.3027 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1066/1786 Training loss: 3.3024 0.0197 sec/batch\n",
      "Epoch 1/1  Iteration 1067/1786 Training loss: 3.3022 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 1068/1786 Training loss: 3.3020 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 1069/1786 Training loss: 3.3019 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1070/1786 Training loss: 3.3017 0.0197 sec/batch\n",
      "Epoch 1/1  Iteration 1071/1786 Training loss: 3.3015 0.0220 sec/batch\n",
      "Epoch 1/1  Iteration 1072/1786 Training loss: 3.3013 0.0211 sec/batch\n",
      "Epoch 1/1  Iteration 1073/1786 Training loss: 3.3011 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 1074/1786 Training loss: 3.3009 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 1075/1786 Training loss: 3.3007 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 1076/1786 Training loss: 3.3005 0.0236 sec/batch\n",
      "Epoch 1/1  Iteration 1077/1786 Training loss: 3.3003 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1078/1786 Training loss: 3.3001 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1079/1786 Training loss: 3.2998 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1080/1786 Training loss: 3.2996 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 1081/1786 Training loss: 3.2994 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 1082/1786 Training loss: 3.2991 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 1083/1786 Training loss: 3.2989 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1084/1786 Training loss: 3.2987 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 1085/1786 Training loss: 3.2985 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 1086/1786 Training loss: 3.2983 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1087/1786 Training loss: 3.2981 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1088/1786 Training loss: 3.2978 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 1089/1786 Training loss: 3.2977 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1090/1786 Training loss: 3.2975 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 1091/1786 Training loss: 3.2973 0.0319 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1092/1786 Training loss: 3.2970 0.0326 sec/batch\n",
      "Epoch 1/1  Iteration 1093/1786 Training loss: 3.2968 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1094/1786 Training loss: 3.2966 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1095/1786 Training loss: 3.2964 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1096/1786 Training loss: 3.2961 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1097/1786 Training loss: 3.2959 0.0376 sec/batch\n",
      "Epoch 1/1  Iteration 1098/1786 Training loss: 3.2958 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1099/1786 Training loss: 3.2956 0.0359 sec/batch\n",
      "Epoch 1/1  Iteration 1100/1786 Training loss: 3.2954 0.0306 sec/batch\n",
      "Validation loss: 3.00382 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1101/1786 Training loss: 3.2952 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1102/1786 Training loss: 3.2950 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1103/1786 Training loss: 3.2948 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1104/1786 Training loss: 3.2946 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 1105/1786 Training loss: 3.2943 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 1106/1786 Training loss: 3.2942 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 1107/1786 Training loss: 3.2939 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1108/1786 Training loss: 3.2937 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 1109/1786 Training loss: 3.2935 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 1110/1786 Training loss: 3.2934 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1111/1786 Training loss: 3.2931 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1112/1786 Training loss: 3.2930 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 1113/1786 Training loss: 3.2928 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1114/1786 Training loss: 3.2926 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 1115/1786 Training loss: 3.2924 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1116/1786 Training loss: 3.2922 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1117/1786 Training loss: 3.2920 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1118/1786 Training loss: 3.2917 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1119/1786 Training loss: 3.2915 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 1120/1786 Training loss: 3.2913 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 1121/1786 Training loss: 3.2911 0.0657 sec/batch\n",
      "Epoch 1/1  Iteration 1122/1786 Training loss: 3.2910 0.0301 sec/batch\n",
      "Epoch 1/1  Iteration 1123/1786 Training loss: 3.2908 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 1124/1786 Training loss: 3.2906 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 1125/1786 Training loss: 3.2904 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 1126/1786 Training loss: 3.2902 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1127/1786 Training loss: 3.2901 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 1128/1786 Training loss: 3.2899 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 1129/1786 Training loss: 3.2897 0.0322 sec/batch\n",
      "Epoch 1/1  Iteration 1130/1786 Training loss: 3.2896 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 1131/1786 Training loss: 3.2894 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1132/1786 Training loss: 3.2892 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 1133/1786 Training loss: 3.2890 0.0401 sec/batch\n",
      "Epoch 1/1  Iteration 1134/1786 Training loss: 3.2888 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1135/1786 Training loss: 3.2887 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1136/1786 Training loss: 3.2885 0.0340 sec/batch\n",
      "Epoch 1/1  Iteration 1137/1786 Training loss: 3.2883 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 1138/1786 Training loss: 3.2881 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1139/1786 Training loss: 3.2879 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1140/1786 Training loss: 3.2877 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1141/1786 Training loss: 3.2876 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1142/1786 Training loss: 3.2874 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 1143/1786 Training loss: 3.2872 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1144/1786 Training loss: 3.2870 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1145/1786 Training loss: 3.2868 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1146/1786 Training loss: 3.2866 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 1147/1786 Training loss: 3.2864 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 1148/1786 Training loss: 3.2862 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 1149/1786 Training loss: 3.2861 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 1150/1786 Training loss: 3.2859 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1151/1786 Training loss: 3.2858 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 1152/1786 Training loss: 3.2856 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1153/1786 Training loss: 3.2854 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 1154/1786 Training loss: 3.2852 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1155/1786 Training loss: 3.2850 0.0322 sec/batch\n",
      "Epoch 1/1  Iteration 1156/1786 Training loss: 3.2848 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1157/1786 Training loss: 3.2846 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1158/1786 Training loss: 3.2844 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 1159/1786 Training loss: 3.2842 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 1160/1786 Training loss: 3.2841 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1161/1786 Training loss: 3.2838 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1162/1786 Training loss: 3.2837 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1163/1786 Training loss: 3.2835 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 1164/1786 Training loss: 3.2833 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 1165/1786 Training loss: 3.2831 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 1166/1786 Training loss: 3.2829 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1167/1786 Training loss: 3.2828 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1168/1786 Training loss: 3.2826 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 1169/1786 Training loss: 3.2824 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 1170/1786 Training loss: 3.2823 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1171/1786 Training loss: 3.2821 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 1172/1786 Training loss: 3.2820 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1173/1786 Training loss: 3.2818 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 1174/1786 Training loss: 3.2816 0.0503 sec/batch\n",
      "Epoch 1/1  Iteration 1175/1786 Training loss: 3.2814 0.0375 sec/batch\n",
      "Epoch 1/1  Iteration 1176/1786 Training loss: 3.2813 0.0403 sec/batch\n",
      "Epoch 1/1  Iteration 1177/1786 Training loss: 3.2811 0.0218 sec/batch\n",
      "Epoch 1/1  Iteration 1178/1786 Training loss: 3.2809 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 1179/1786 Training loss: 3.2807 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1180/1786 Training loss: 3.2805 0.0374 sec/batch\n",
      "Epoch 1/1  Iteration 1181/1786 Training loss: 3.2803 0.0236 sec/batch\n",
      "Epoch 1/1  Iteration 1182/1786 Training loss: 3.2801 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1183/1786 Training loss: 3.2800 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 1184/1786 Training loss: 3.2798 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1185/1786 Training loss: 3.2796 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 1186/1786 Training loss: 3.2795 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 1187/1786 Training loss: 3.2793 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1188/1786 Training loss: 3.2791 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1189/1786 Training loss: 3.2789 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 1190/1786 Training loss: 3.2787 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 1191/1786 Training loss: 3.2785 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 1192/1786 Training loss: 3.2783 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 1193/1786 Training loss: 3.2781 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1194/1786 Training loss: 3.2780 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 1195/1786 Training loss: 3.2778 0.0208 sec/batch\n",
      "Epoch 1/1  Iteration 1196/1786 Training loss: 3.2776 0.0197 sec/batch\n",
      "Epoch 1/1  Iteration 1197/1786 Training loss: 3.2774 0.0193 sec/batch\n",
      "Epoch 1/1  Iteration 1198/1786 Training loss: 3.2772 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1199/1786 Training loss: 3.2770 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1200/1786 Training loss: 3.2768 0.0238 sec/batch\n",
      "Validation loss: 2.98675 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1201/1786 Training loss: 3.2766 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1202/1786 Training loss: 3.2764 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 1203/1786 Training loss: 3.2762 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1204/1786 Training loss: 3.2760 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 1205/1786 Training loss: 3.2759 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1206/1786 Training loss: 3.2757 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 1207/1786 Training loss: 3.2754 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1208/1786 Training loss: 3.2752 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 1209/1786 Training loss: 3.2750 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1210/1786 Training loss: 3.2749 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1211/1786 Training loss: 3.2747 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 1212/1786 Training loss: 3.2746 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 1213/1786 Training loss: 3.2745 0.0235 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1214/1786 Training loss: 3.2743 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 1215/1786 Training loss: 3.2742 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 1216/1786 Training loss: 3.2741 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1217/1786 Training loss: 3.2739 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 1218/1786 Training loss: 3.2738 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1219/1786 Training loss: 3.2737 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 1220/1786 Training loss: 3.2735 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1221/1786 Training loss: 3.2734 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 1222/1786 Training loss: 3.2732 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1223/1786 Training loss: 3.2731 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1224/1786 Training loss: 3.2729 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 1225/1786 Training loss: 3.2728 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 1226/1786 Training loss: 3.2726 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 1227/1786 Training loss: 3.2725 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 1228/1786 Training loss: 3.2723 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 1229/1786 Training loss: 3.2721 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1230/1786 Training loss: 3.2720 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1231/1786 Training loss: 3.2719 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 1232/1786 Training loss: 3.2717 0.0218 sec/batch\n",
      "Epoch 1/1  Iteration 1233/1786 Training loss: 3.2716 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 1234/1786 Training loss: 3.2715 0.0208 sec/batch\n",
      "Epoch 1/1  Iteration 1235/1786 Training loss: 3.2713 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 1236/1786 Training loss: 3.2711 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1237/1786 Training loss: 3.2710 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1238/1786 Training loss: 3.2708 0.0204 sec/batch\n",
      "Epoch 1/1  Iteration 1239/1786 Training loss: 3.2706 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1240/1786 Training loss: 3.2704 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 1241/1786 Training loss: 3.2702 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 1242/1786 Training loss: 3.2700 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 1243/1786 Training loss: 3.2699 0.0223 sec/batch\n",
      "Epoch 1/1  Iteration 1244/1786 Training loss: 3.2697 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1245/1786 Training loss: 3.2695 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1246/1786 Training loss: 3.2694 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1247/1786 Training loss: 3.2692 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 1248/1786 Training loss: 3.2691 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1249/1786 Training loss: 3.2690 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 1250/1786 Training loss: 3.2688 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 1251/1786 Training loss: 3.2686 0.0666 sec/batch\n",
      "Epoch 1/1  Iteration 1252/1786 Training loss: 3.2684 0.0440 sec/batch\n",
      "Epoch 1/1  Iteration 1253/1786 Training loss: 3.2683 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 1254/1786 Training loss: 3.2682 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1255/1786 Training loss: 3.2680 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1256/1786 Training loss: 3.2678 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1257/1786 Training loss: 3.2677 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1258/1786 Training loss: 3.2675 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1259/1786 Training loss: 3.2673 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1260/1786 Training loss: 3.2672 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1261/1786 Training loss: 3.2670 0.0360 sec/batch\n",
      "Epoch 1/1  Iteration 1262/1786 Training loss: 3.2668 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 1263/1786 Training loss: 3.2666 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1264/1786 Training loss: 3.2665 0.0379 sec/batch\n",
      "Epoch 1/1  Iteration 1265/1786 Training loss: 3.2663 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1266/1786 Training loss: 3.2661 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 1267/1786 Training loss: 3.2659 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1268/1786 Training loss: 3.2657 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1269/1786 Training loss: 3.2656 0.0347 sec/batch\n",
      "Epoch 1/1  Iteration 1270/1786 Training loss: 3.2654 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1271/1786 Training loss: 3.2652 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1272/1786 Training loss: 3.2651 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1273/1786 Training loss: 3.2649 0.0359 sec/batch\n",
      "Epoch 1/1  Iteration 1274/1786 Training loss: 3.2647 0.0387 sec/batch\n",
      "Epoch 1/1  Iteration 1275/1786 Training loss: 3.2645 0.0373 sec/batch\n",
      "Epoch 1/1  Iteration 1276/1786 Training loss: 3.2643 0.0472 sec/batch\n",
      "Epoch 1/1  Iteration 1277/1786 Training loss: 3.2642 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1278/1786 Training loss: 3.2640 0.0338 sec/batch\n",
      "Epoch 1/1  Iteration 1279/1786 Training loss: 3.2638 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 1280/1786 Training loss: 3.2636 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1281/1786 Training loss: 3.2635 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1282/1786 Training loss: 3.2633 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1283/1786 Training loss: 3.2631 0.0370 sec/batch\n",
      "Epoch 1/1  Iteration 1284/1786 Training loss: 3.2630 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 1285/1786 Training loss: 3.2628 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1286/1786 Training loss: 3.2627 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1287/1786 Training loss: 3.2625 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1288/1786 Training loss: 3.2623 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 1289/1786 Training loss: 3.2622 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 1290/1786 Training loss: 3.2621 0.0366 sec/batch\n",
      "Epoch 1/1  Iteration 1291/1786 Training loss: 3.2619 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1292/1786 Training loss: 3.2617 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 1293/1786 Training loss: 3.2616 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1294/1786 Training loss: 3.2614 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1295/1786 Training loss: 3.2612 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 1296/1786 Training loss: 3.2610 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1297/1786 Training loss: 3.2609 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 1298/1786 Training loss: 3.2607 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 1299/1786 Training loss: 3.2605 0.0360 sec/batch\n",
      "Epoch 1/1  Iteration 1300/1786 Training loss: 3.2604 0.0399 sec/batch\n",
      "Validation loss: 2.9692 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1301/1786 Training loss: 3.2603 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 1302/1786 Training loss: 3.2601 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1303/1786 Training loss: 3.2599 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 1304/1786 Training loss: 3.2597 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 1305/1786 Training loss: 3.2596 0.0342 sec/batch\n",
      "Epoch 1/1  Iteration 1306/1786 Training loss: 3.2595 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 1307/1786 Training loss: 3.2593 0.0342 sec/batch\n",
      "Epoch 1/1  Iteration 1308/1786 Training loss: 3.2591 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1309/1786 Training loss: 3.2590 0.0346 sec/batch\n",
      "Epoch 1/1  Iteration 1310/1786 Training loss: 3.2588 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 1311/1786 Training loss: 3.2587 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1312/1786 Training loss: 3.2585 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1313/1786 Training loss: 3.2583 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1314/1786 Training loss: 3.2582 0.0363 sec/batch\n",
      "Epoch 1/1  Iteration 1315/1786 Training loss: 3.2580 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 1316/1786 Training loss: 3.2578 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 1317/1786 Training loss: 3.2577 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1318/1786 Training loss: 3.2575 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1319/1786 Training loss: 3.2573 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 1320/1786 Training loss: 3.2571 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1321/1786 Training loss: 3.2570 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 1322/1786 Training loss: 3.2568 0.0356 sec/batch\n",
      "Epoch 1/1  Iteration 1323/1786 Training loss: 3.2567 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1324/1786 Training loss: 3.2565 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 1325/1786 Training loss: 3.2564 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1326/1786 Training loss: 3.2562 0.0223 sec/batch\n",
      "Epoch 1/1  Iteration 1327/1786 Training loss: 3.2561 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 1328/1786 Training loss: 3.2559 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 1329/1786 Training loss: 3.2557 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 1330/1786 Training loss: 3.2555 0.0330 sec/batch\n",
      "Epoch 1/1  Iteration 1331/1786 Training loss: 3.2554 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 1332/1786 Training loss: 3.2552 0.0354 sec/batch\n",
      "Epoch 1/1  Iteration 1333/1786 Training loss: 3.2551 0.0380 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1334/1786 Training loss: 3.2549 0.0348 sec/batch\n",
      "Epoch 1/1  Iteration 1335/1786 Training loss: 3.2548 0.0368 sec/batch\n",
      "Epoch 1/1  Iteration 1336/1786 Training loss: 3.2546 0.0342 sec/batch\n",
      "Epoch 1/1  Iteration 1337/1786 Training loss: 3.2544 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 1338/1786 Training loss: 3.2543 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 1339/1786 Training loss: 3.2541 0.0216 sec/batch\n",
      "Epoch 1/1  Iteration 1340/1786 Training loss: 3.2540 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1341/1786 Training loss: 3.2538 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1342/1786 Training loss: 3.2537 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1343/1786 Training loss: 3.2535 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1344/1786 Training loss: 3.2534 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1345/1786 Training loss: 3.2533 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1346/1786 Training loss: 3.2532 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 1347/1786 Training loss: 3.2530 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 1348/1786 Training loss: 3.2529 0.0207 sec/batch\n",
      "Epoch 1/1  Iteration 1349/1786 Training loss: 3.2527 0.0388 sec/batch\n",
      "Epoch 1/1  Iteration 1350/1786 Training loss: 3.2526 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1351/1786 Training loss: 3.2525 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 1352/1786 Training loss: 3.2523 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 1353/1786 Training loss: 3.2522 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 1354/1786 Training loss: 3.2520 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1355/1786 Training loss: 3.2519 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 1356/1786 Training loss: 3.2518 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 1357/1786 Training loss: 3.2516 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 1358/1786 Training loss: 3.2515 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 1359/1786 Training loss: 3.2514 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 1360/1786 Training loss: 3.2512 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1361/1786 Training loss: 3.2511 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1362/1786 Training loss: 3.2509 0.0372 sec/batch\n",
      "Epoch 1/1  Iteration 1363/1786 Training loss: 3.2507 0.0647 sec/batch\n",
      "Epoch 1/1  Iteration 1364/1786 Training loss: 3.2506 0.0345 sec/batch\n",
      "Epoch 1/1  Iteration 1365/1786 Training loss: 3.2504 0.0324 sec/batch\n",
      "Epoch 1/1  Iteration 1366/1786 Training loss: 3.2502 0.0329 sec/batch\n",
      "Epoch 1/1  Iteration 1367/1786 Training loss: 3.2501 0.0306 sec/batch\n",
      "Epoch 1/1  Iteration 1368/1786 Training loss: 3.2500 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 1369/1786 Training loss: 3.2499 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 1370/1786 Training loss: 3.2497 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 1371/1786 Training loss: 3.2496 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 1372/1786 Training loss: 3.2495 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1373/1786 Training loss: 3.2494 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1374/1786 Training loss: 3.2492 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1375/1786 Training loss: 3.2491 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1376/1786 Training loss: 3.2489 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 1377/1786 Training loss: 3.2488 0.0216 sec/batch\n",
      "Epoch 1/1  Iteration 1378/1786 Training loss: 3.2486 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1379/1786 Training loss: 3.2485 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 1380/1786 Training loss: 3.2483 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1381/1786 Training loss: 3.2482 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 1382/1786 Training loss: 3.2481 0.0361 sec/batch\n",
      "Epoch 1/1  Iteration 1383/1786 Training loss: 3.2479 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1384/1786 Training loss: 3.2478 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1385/1786 Training loss: 3.2477 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 1386/1786 Training loss: 3.2475 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 1387/1786 Training loss: 3.2474 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 1388/1786 Training loss: 3.2472 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1389/1786 Training loss: 3.2471 0.0336 sec/batch\n",
      "Epoch 1/1  Iteration 1390/1786 Training loss: 3.2469 0.0336 sec/batch\n",
      "Epoch 1/1  Iteration 1391/1786 Training loss: 3.2467 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1392/1786 Training loss: 3.2466 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1393/1786 Training loss: 3.2464 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1394/1786 Training loss: 3.2463 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1395/1786 Training loss: 3.2461 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1396/1786 Training loss: 3.2459 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1397/1786 Training loss: 3.2457 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1398/1786 Training loss: 3.2456 0.0267 sec/batch\n",
      "Epoch 1/1  Iteration 1399/1786 Training loss: 3.2455 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1400/1786 Training loss: 3.2453 0.0274 sec/batch\n",
      "Validation loss: 2.95361 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1401/1786 Training loss: 3.2452 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1402/1786 Training loss: 3.2450 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1403/1786 Training loss: 3.2449 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1404/1786 Training loss: 3.2448 0.0326 sec/batch\n",
      "Epoch 1/1  Iteration 1405/1786 Training loss: 3.2446 0.0336 sec/batch\n",
      "Epoch 1/1  Iteration 1406/1786 Training loss: 3.2444 0.0332 sec/batch\n",
      "Epoch 1/1  Iteration 1407/1786 Training loss: 3.2443 0.0378 sec/batch\n",
      "Epoch 1/1  Iteration 1408/1786 Training loss: 3.2442 0.0324 sec/batch\n",
      "Epoch 1/1  Iteration 1409/1786 Training loss: 3.2440 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1410/1786 Training loss: 3.2439 0.0463 sec/batch\n",
      "Epoch 1/1  Iteration 1411/1786 Training loss: 3.2438 0.0405 sec/batch\n",
      "Epoch 1/1  Iteration 1412/1786 Training loss: 3.2436 0.0380 sec/batch\n",
      "Epoch 1/1  Iteration 1413/1786 Training loss: 3.2434 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1414/1786 Training loss: 3.2433 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1415/1786 Training loss: 3.2432 0.0323 sec/batch\n",
      "Epoch 1/1  Iteration 1416/1786 Training loss: 3.2430 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1417/1786 Training loss: 3.2429 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 1418/1786 Training loss: 3.2427 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 1419/1786 Training loss: 3.2426 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1420/1786 Training loss: 3.2424 0.0335 sec/batch\n",
      "Epoch 1/1  Iteration 1421/1786 Training loss: 3.2423 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1422/1786 Training loss: 3.2422 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1423/1786 Training loss: 3.2420 0.0308 sec/batch\n",
      "Epoch 1/1  Iteration 1424/1786 Training loss: 3.2419 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 1425/1786 Training loss: 3.2417 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1426/1786 Training loss: 3.2416 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 1427/1786 Training loss: 3.2414 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 1428/1786 Training loss: 3.2413 0.0215 sec/batch\n",
      "Epoch 1/1  Iteration 1429/1786 Training loss: 3.2411 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 1430/1786 Training loss: 3.2409 0.0257 sec/batch\n",
      "Epoch 1/1  Iteration 1431/1786 Training loss: 3.2408 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1432/1786 Training loss: 3.2407 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1433/1786 Training loss: 3.2405 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 1434/1786 Training loss: 3.2403 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 1435/1786 Training loss: 3.2402 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1436/1786 Training loss: 3.2400 0.0202 sec/batch\n",
      "Epoch 1/1  Iteration 1437/1786 Training loss: 3.2399 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 1438/1786 Training loss: 3.2397 0.0249 sec/batch\n",
      "Epoch 1/1  Iteration 1439/1786 Training loss: 3.2396 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 1440/1786 Training loss: 3.2394 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1441/1786 Training loss: 3.2393 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 1442/1786 Training loss: 3.2392 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1443/1786 Training loss: 3.2390 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 1444/1786 Training loss: 3.2389 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 1445/1786 Training loss: 3.2388 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1446/1786 Training loss: 3.2387 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1447/1786 Training loss: 3.2385 0.0204 sec/batch\n",
      "Epoch 1/1  Iteration 1448/1786 Training loss: 3.2383 0.0260 sec/batch\n",
      "Epoch 1/1  Iteration 1449/1786 Training loss: 3.2382 0.0449 sec/batch\n",
      "Epoch 1/1  Iteration 1450/1786 Training loss: 3.2380 0.0223 sec/batch\n",
      "Epoch 1/1  Iteration 1451/1786 Training loss: 3.2379 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 1452/1786 Training loss: 3.2377 0.0348 sec/batch\n",
      "Epoch 1/1  Iteration 1453/1786 Training loss: 3.2376 0.0317 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1454/1786 Training loss: 3.2375 0.0365 sec/batch\n",
      "Epoch 1/1  Iteration 1455/1786 Training loss: 3.2373 0.0353 sec/batch\n",
      "Epoch 1/1  Iteration 1456/1786 Training loss: 3.2372 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1457/1786 Training loss: 3.2371 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 1458/1786 Training loss: 3.2369 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 1459/1786 Training loss: 3.2367 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 1460/1786 Training loss: 3.2366 0.0221 sec/batch\n",
      "Epoch 1/1  Iteration 1461/1786 Training loss: 3.2365 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 1462/1786 Training loss: 3.2363 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1463/1786 Training loss: 3.2361 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1464/1786 Training loss: 3.2360 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1465/1786 Training loss: 3.2358 0.0453 sec/batch\n",
      "Epoch 1/1  Iteration 1466/1786 Training loss: 3.2357 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1467/1786 Training loss: 3.2356 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1468/1786 Training loss: 3.2354 0.0329 sec/batch\n",
      "Epoch 1/1  Iteration 1469/1786 Training loss: 3.2353 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1470/1786 Training loss: 3.2351 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 1471/1786 Training loss: 3.2349 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1472/1786 Training loss: 3.2348 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1473/1786 Training loss: 3.2346 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 1474/1786 Training loss: 3.2344 0.0277 sec/batch\n",
      "Epoch 1/1  Iteration 1475/1786 Training loss: 3.2343 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 1476/1786 Training loss: 3.2342 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1477/1786 Training loss: 3.2340 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1478/1786 Training loss: 3.2339 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 1479/1786 Training loss: 3.2337 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1480/1786 Training loss: 3.2336 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1481/1786 Training loss: 3.2335 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 1482/1786 Training loss: 3.2333 0.0236 sec/batch\n",
      "Epoch 1/1  Iteration 1483/1786 Training loss: 3.2332 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 1484/1786 Training loss: 3.2331 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1485/1786 Training loss: 3.2329 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1486/1786 Training loss: 3.2327 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 1487/1786 Training loss: 3.2326 0.0222 sec/batch\n",
      "Epoch 1/1  Iteration 1488/1786 Training loss: 3.2324 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 1489/1786 Training loss: 3.2323 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1490/1786 Training loss: 3.2321 0.0375 sec/batch\n",
      "Epoch 1/1  Iteration 1491/1786 Training loss: 3.2319 0.0335 sec/batch\n",
      "Epoch 1/1  Iteration 1492/1786 Training loss: 3.2318 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 1493/1786 Training loss: 3.2317 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1494/1786 Training loss: 3.2316 0.0422 sec/batch\n",
      "Epoch 1/1  Iteration 1495/1786 Training loss: 3.2315 0.0360 sec/batch\n",
      "Epoch 1/1  Iteration 1496/1786 Training loss: 3.2313 0.0334 sec/batch\n",
      "Epoch 1/1  Iteration 1497/1786 Training loss: 3.2312 0.0384 sec/batch\n",
      "Epoch 1/1  Iteration 1498/1786 Training loss: 3.2310 0.0390 sec/batch\n",
      "Epoch 1/1  Iteration 1499/1786 Training loss: 3.2309 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 1500/1786 Training loss: 3.2307 0.0292 sec/batch\n",
      "Validation loss: 2.9363 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1501/1786 Training loss: 3.2306 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1502/1786 Training loss: 3.2305 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 1503/1786 Training loss: 3.2304 0.0211 sec/batch\n",
      "Epoch 1/1  Iteration 1504/1786 Training loss: 3.2302 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 1505/1786 Training loss: 3.2301 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 1506/1786 Training loss: 3.2300 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1507/1786 Training loss: 3.2298 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 1508/1786 Training loss: 3.2297 0.0206 sec/batch\n",
      "Epoch 1/1  Iteration 1509/1786 Training loss: 3.2295 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1510/1786 Training loss: 3.2294 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1511/1786 Training loss: 3.2292 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1512/1786 Training loss: 3.2291 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1513/1786 Training loss: 3.2289 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 1514/1786 Training loss: 3.2288 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1515/1786 Training loss: 3.2287 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1516/1786 Training loss: 3.2285 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1517/1786 Training loss: 3.2284 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 1518/1786 Training loss: 3.2282 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1519/1786 Training loss: 3.2281 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1520/1786 Training loss: 3.2279 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 1521/1786 Training loss: 3.2278 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1522/1786 Training loss: 3.2276 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1523/1786 Training loss: 3.2275 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 1524/1786 Training loss: 3.2274 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 1525/1786 Training loss: 3.2272 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1526/1786 Training loss: 3.2270 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1527/1786 Training loss: 3.2269 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 1528/1786 Training loss: 3.2268 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 1529/1786 Training loss: 3.2266 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1530/1786 Training loss: 3.2265 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 1531/1786 Training loss: 3.2264 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1532/1786 Training loss: 3.2262 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 1533/1786 Training loss: 3.2261 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1534/1786 Training loss: 3.2260 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1535/1786 Training loss: 3.2258 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 1536/1786 Training loss: 3.2257 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 1537/1786 Training loss: 3.2255 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1538/1786 Training loss: 3.2254 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 1539/1786 Training loss: 3.2253 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1540/1786 Training loss: 3.2252 0.0241 sec/batch\n",
      "Epoch 1/1  Iteration 1541/1786 Training loss: 3.2251 0.0226 sec/batch\n",
      "Epoch 1/1  Iteration 1542/1786 Training loss: 3.2250 0.0247 sec/batch\n",
      "Epoch 1/1  Iteration 1543/1786 Training loss: 3.2248 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 1544/1786 Training loss: 3.2247 0.0230 sec/batch\n",
      "Epoch 1/1  Iteration 1545/1786 Training loss: 3.2246 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1546/1786 Training loss: 3.2245 0.0202 sec/batch\n",
      "Epoch 1/1  Iteration 1547/1786 Training loss: 3.2243 0.0262 sec/batch\n",
      "Epoch 1/1  Iteration 1548/1786 Training loss: 3.2242 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1549/1786 Training loss: 3.2240 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 1550/1786 Training loss: 3.2239 0.0216 sec/batch\n",
      "Epoch 1/1  Iteration 1551/1786 Training loss: 3.2238 0.0344 sec/batch\n",
      "Epoch 1/1  Iteration 1552/1786 Training loss: 3.2236 0.0219 sec/batch\n",
      "Epoch 1/1  Iteration 1553/1786 Training loss: 3.2235 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 1554/1786 Training loss: 3.2234 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1555/1786 Training loss: 3.2233 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 1556/1786 Training loss: 3.2231 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1557/1786 Training loss: 3.2230 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1558/1786 Training loss: 3.2229 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1559/1786 Training loss: 3.2228 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1560/1786 Training loss: 3.2226 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 1561/1786 Training loss: 3.2225 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1562/1786 Training loss: 3.2224 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 1563/1786 Training loss: 3.2222 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 1564/1786 Training loss: 3.2221 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1565/1786 Training loss: 3.2220 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1566/1786 Training loss: 3.2219 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1567/1786 Training loss: 3.2217 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 1568/1786 Training loss: 3.2216 0.0208 sec/batch\n",
      "Epoch 1/1  Iteration 1569/1786 Training loss: 3.2215 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 1570/1786 Training loss: 3.2213 0.0231 sec/batch\n",
      "Epoch 1/1  Iteration 1571/1786 Training loss: 3.2212 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1572/1786 Training loss: 3.2211 0.0229 sec/batch\n",
      "Epoch 1/1  Iteration 1573/1786 Training loss: 3.2210 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1574/1786 Training loss: 3.2208 0.0224 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1575/1786 Training loss: 3.2207 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 1576/1786 Training loss: 3.2206 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1577/1786 Training loss: 3.2204 0.0224 sec/batch\n",
      "Epoch 1/1  Iteration 1578/1786 Training loss: 3.2202 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 1579/1786 Training loss: 3.2201 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1580/1786 Training loss: 3.2200 0.0228 sec/batch\n",
      "Epoch 1/1  Iteration 1581/1786 Training loss: 3.2198 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 1582/1786 Training loss: 3.2197 0.0440 sec/batch\n",
      "Epoch 1/1  Iteration 1583/1786 Training loss: 3.2196 0.0333 sec/batch\n",
      "Epoch 1/1  Iteration 1584/1786 Training loss: 3.2194 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 1585/1786 Training loss: 3.2193 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 1586/1786 Training loss: 3.2191 0.0330 sec/batch\n",
      "Epoch 1/1  Iteration 1587/1786 Training loss: 3.2190 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 1588/1786 Training loss: 3.2189 0.0429 sec/batch\n",
      "Epoch 1/1  Iteration 1589/1786 Training loss: 3.2187 0.0303 sec/batch\n",
      "Epoch 1/1  Iteration 1590/1786 Training loss: 3.2186 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 1591/1786 Training loss: 3.2184 0.0352 sec/batch\n",
      "Epoch 1/1  Iteration 1592/1786 Training loss: 3.2183 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1593/1786 Training loss: 3.2182 0.0237 sec/batch\n",
      "Epoch 1/1  Iteration 1594/1786 Training loss: 3.2181 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 1595/1786 Training loss: 3.2180 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1596/1786 Training loss: 3.2178 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1597/1786 Training loss: 3.2177 0.0264 sec/batch\n",
      "Epoch 1/1  Iteration 1598/1786 Training loss: 3.2176 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1599/1786 Training loss: 3.2175 0.0444 sec/batch\n",
      "Epoch 1/1  Iteration 1600/1786 Training loss: 3.2174 0.0407 sec/batch\n",
      "Validation loss: 2.92095 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1601/1786 Training loss: 3.2173 0.0309 sec/batch\n",
      "Epoch 1/1  Iteration 1602/1786 Training loss: 3.2171 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1603/1786 Training loss: 3.2170 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1604/1786 Training loss: 3.2168 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 1605/1786 Training loss: 3.2167 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 1606/1786 Training loss: 3.2166 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1607/1786 Training loss: 3.2164 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1608/1786 Training loss: 3.2163 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 1609/1786 Training loss: 3.2162 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1610/1786 Training loss: 3.2161 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 1611/1786 Training loss: 3.2160 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 1612/1786 Training loss: 3.2159 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1613/1786 Training loss: 3.2157 0.0244 sec/batch\n",
      "Epoch 1/1  Iteration 1614/1786 Training loss: 3.2156 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 1615/1786 Training loss: 3.2155 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1616/1786 Training loss: 3.2153 0.0313 sec/batch\n",
      "Epoch 1/1  Iteration 1617/1786 Training loss: 3.2152 0.0329 sec/batch\n",
      "Epoch 1/1  Iteration 1618/1786 Training loss: 3.2151 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1619/1786 Training loss: 3.2150 0.0289 sec/batch\n",
      "Epoch 1/1  Iteration 1620/1786 Training loss: 3.2148 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1621/1786 Training loss: 3.2147 0.0389 sec/batch\n",
      "Epoch 1/1  Iteration 1622/1786 Training loss: 3.2146 0.0353 sec/batch\n",
      "Epoch 1/1  Iteration 1623/1786 Training loss: 3.2145 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 1624/1786 Training loss: 3.2143 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1625/1786 Training loss: 3.2142 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 1626/1786 Training loss: 3.2141 0.0307 sec/batch\n",
      "Epoch 1/1  Iteration 1627/1786 Training loss: 3.2140 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1628/1786 Training loss: 3.2138 0.0292 sec/batch\n",
      "Epoch 1/1  Iteration 1629/1786 Training loss: 3.2137 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 1630/1786 Training loss: 3.2135 0.0196 sec/batch\n",
      "Epoch 1/1  Iteration 1631/1786 Training loss: 3.2134 0.0213 sec/batch\n",
      "Epoch 1/1  Iteration 1632/1786 Training loss: 3.2132 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1633/1786 Training loss: 3.2131 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 1634/1786 Training loss: 3.2129 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1635/1786 Training loss: 3.2128 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1636/1786 Training loss: 3.2126 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 1637/1786 Training loss: 3.2125 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1638/1786 Training loss: 3.2123 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 1639/1786 Training loss: 3.2122 0.0369 sec/batch\n",
      "Epoch 1/1  Iteration 1640/1786 Training loss: 3.2121 0.0337 sec/batch\n",
      "Epoch 1/1  Iteration 1641/1786 Training loss: 3.2120 0.0331 sec/batch\n",
      "Epoch 1/1  Iteration 1642/1786 Training loss: 3.2118 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1643/1786 Training loss: 3.2118 0.0324 sec/batch\n",
      "Epoch 1/1  Iteration 1644/1786 Training loss: 3.2116 0.0207 sec/batch\n",
      "Epoch 1/1  Iteration 1645/1786 Training loss: 3.2115 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1646/1786 Training loss: 3.2114 0.0233 sec/batch\n",
      "Epoch 1/1  Iteration 1647/1786 Training loss: 3.2113 0.0279 sec/batch\n",
      "Epoch 1/1  Iteration 1648/1786 Training loss: 3.2111 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 1649/1786 Training loss: 3.2110 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 1650/1786 Training loss: 3.2108 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1651/1786 Training loss: 3.2107 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1652/1786 Training loss: 3.2106 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 1653/1786 Training loss: 3.2105 0.0327 sec/batch\n",
      "Epoch 1/1  Iteration 1654/1786 Training loss: 3.2104 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 1655/1786 Training loss: 3.2103 0.0342 sec/batch\n",
      "Epoch 1/1  Iteration 1656/1786 Training loss: 3.2102 0.0341 sec/batch\n",
      "Epoch 1/1  Iteration 1657/1786 Training loss: 3.2100 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1658/1786 Training loss: 3.2099 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1659/1786 Training loss: 3.2098 0.0325 sec/batch\n",
      "Epoch 1/1  Iteration 1660/1786 Training loss: 3.2097 0.0268 sec/batch\n",
      "Epoch 1/1  Iteration 1661/1786 Training loss: 3.2096 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1662/1786 Training loss: 3.2095 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 1663/1786 Training loss: 3.2094 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1664/1786 Training loss: 3.2093 0.0225 sec/batch\n",
      "Epoch 1/1  Iteration 1665/1786 Training loss: 3.2092 0.0294 sec/batch\n",
      "Epoch 1/1  Iteration 1666/1786 Training loss: 3.2091 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1667/1786 Training loss: 3.2090 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1668/1786 Training loss: 3.2088 0.0253 sec/batch\n",
      "Epoch 1/1  Iteration 1669/1786 Training loss: 3.2087 0.0318 sec/batch\n",
      "Epoch 1/1  Iteration 1670/1786 Training loss: 3.2086 0.0298 sec/batch\n",
      "Epoch 1/1  Iteration 1671/1786 Training loss: 3.2084 0.0275 sec/batch\n",
      "Epoch 1/1  Iteration 1672/1786 Training loss: 3.2083 0.0238 sec/batch\n",
      "Epoch 1/1  Iteration 1673/1786 Training loss: 3.2082 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1674/1786 Training loss: 3.2081 0.0305 sec/batch\n",
      "Epoch 1/1  Iteration 1675/1786 Training loss: 3.2080 0.0255 sec/batch\n",
      "Epoch 1/1  Iteration 1676/1786 Training loss: 3.2078 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 1677/1786 Training loss: 3.2078 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1678/1786 Training loss: 3.2076 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1679/1786 Training loss: 3.2075 0.0302 sec/batch\n",
      "Epoch 1/1  Iteration 1680/1786 Training loss: 3.2074 0.0240 sec/batch\n",
      "Epoch 1/1  Iteration 1681/1786 Training loss: 3.2073 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 1682/1786 Training loss: 3.2072 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 1683/1786 Training loss: 3.2071 0.0310 sec/batch\n",
      "Epoch 1/1  Iteration 1684/1786 Training loss: 3.2070 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1685/1786 Training loss: 3.2068 0.0271 sec/batch\n",
      "Epoch 1/1  Iteration 1686/1786 Training loss: 3.2067 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 1687/1786 Training loss: 3.2066 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1688/1786 Training loss: 3.2065 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1689/1786 Training loss: 3.2063 0.0278 sec/batch\n",
      "Epoch 1/1  Iteration 1690/1786 Training loss: 3.2062 0.0285 sec/batch\n",
      "Epoch 1/1  Iteration 1691/1786 Training loss: 3.2060 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1692/1786 Training loss: 3.2059 0.0248 sec/batch\n",
      "Epoch 1/1  Iteration 1693/1786 Training loss: 3.2058 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 1694/1786 Training loss: 3.2057 0.0215 sec/batch\n",
      "Epoch 1/1  Iteration 1695/1786 Training loss: 3.2056 0.0245 sec/batch\n",
      "Epoch 1/1  Iteration 1696/1786 Training loss: 3.2055 0.0281 sec/batch\n",
      "Epoch 1/1  Iteration 1697/1786 Training loss: 3.2054 0.0247 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1698/1786 Training loss: 3.2053 0.0317 sec/batch\n",
      "Epoch 1/1  Iteration 1699/1786 Training loss: 3.2052 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1700/1786 Training loss: 3.2051 0.0307 sec/batch\n",
      "Validation loss: 2.90725 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 1701/1786 Training loss: 3.2050 0.0274 sec/batch\n",
      "Epoch 1/1  Iteration 1702/1786 Training loss: 3.2049 0.0265 sec/batch\n",
      "Epoch 1/1  Iteration 1703/1786 Training loss: 3.2048 0.0252 sec/batch\n",
      "Epoch 1/1  Iteration 1704/1786 Training loss: 3.2047 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1705/1786 Training loss: 3.2046 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1706/1786 Training loss: 3.2044 0.0276 sec/batch\n",
      "Epoch 1/1  Iteration 1707/1786 Training loss: 3.2043 0.0269 sec/batch\n",
      "Epoch 1/1  Iteration 1708/1786 Training loss: 3.2042 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1709/1786 Training loss: 3.2041 0.0214 sec/batch\n",
      "Epoch 1/1  Iteration 1710/1786 Training loss: 3.2040 0.0210 sec/batch\n",
      "Epoch 1/1  Iteration 1711/1786 Training loss: 3.2039 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 1712/1786 Training loss: 3.2038 0.0295 sec/batch\n",
      "Epoch 1/1  Iteration 1713/1786 Training loss: 3.2037 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1714/1786 Training loss: 3.2036 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 1715/1786 Training loss: 3.2035 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1716/1786 Training loss: 3.2034 0.0212 sec/batch\n",
      "Epoch 1/1  Iteration 1717/1786 Training loss: 3.2033 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 1718/1786 Training loss: 3.2032 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 1719/1786 Training loss: 3.2031 0.0254 sec/batch\n",
      "Epoch 1/1  Iteration 1720/1786 Training loss: 3.2030 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 1721/1786 Training loss: 3.2028 0.0239 sec/batch\n",
      "Epoch 1/1  Iteration 1722/1786 Training loss: 3.2027 0.0243 sec/batch\n",
      "Epoch 1/1  Iteration 1723/1786 Training loss: 3.2026 0.0246 sec/batch\n",
      "Epoch 1/1  Iteration 1724/1786 Training loss: 3.2024 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 1725/1786 Training loss: 3.2023 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 1726/1786 Training loss: 3.2022 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1727/1786 Training loss: 3.2020 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1728/1786 Training loss: 3.2019 0.0218 sec/batch\n",
      "Epoch 1/1  Iteration 1729/1786 Training loss: 3.2018 0.0235 sec/batch\n",
      "Epoch 1/1  Iteration 1730/1786 Training loss: 3.2017 0.0204 sec/batch\n",
      "Epoch 1/1  Iteration 1731/1786 Training loss: 3.2016 0.0203 sec/batch\n",
      "Epoch 1/1  Iteration 1732/1786 Training loss: 3.2015 0.0320 sec/batch\n",
      "Epoch 1/1  Iteration 1733/1786 Training loss: 3.2014 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1734/1786 Training loss: 3.2013 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1735/1786 Training loss: 3.2012 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 1736/1786 Training loss: 3.2011 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1737/1786 Training loss: 3.2010 0.0304 sec/batch\n",
      "Epoch 1/1  Iteration 1738/1786 Training loss: 3.2009 0.0321 sec/batch\n",
      "Epoch 1/1  Iteration 1739/1786 Training loss: 3.2007 0.0288 sec/batch\n",
      "Epoch 1/1  Iteration 1740/1786 Training loss: 3.2007 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1741/1786 Training loss: 3.2005 0.0250 sec/batch\n",
      "Epoch 1/1  Iteration 1742/1786 Training loss: 3.2004 0.0284 sec/batch\n",
      "Epoch 1/1  Iteration 1743/1786 Training loss: 3.2003 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1744/1786 Training loss: 3.2002 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1745/1786 Training loss: 3.2001 0.0297 sec/batch\n",
      "Epoch 1/1  Iteration 1746/1786 Training loss: 3.1999 0.0261 sec/batch\n",
      "Epoch 1/1  Iteration 1747/1786 Training loss: 3.1998 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 1748/1786 Training loss: 3.1997 0.0344 sec/batch\n",
      "Epoch 1/1  Iteration 1749/1786 Training loss: 3.1996 0.0311 sec/batch\n",
      "Epoch 1/1  Iteration 1750/1786 Training loss: 3.1995 0.0283 sec/batch\n",
      "Epoch 1/1  Iteration 1751/1786 Training loss: 3.1993 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1752/1786 Training loss: 3.1992 0.0204 sec/batch\n",
      "Epoch 1/1  Iteration 1753/1786 Training loss: 3.1991 0.0290 sec/batch\n",
      "Epoch 1/1  Iteration 1754/1786 Training loss: 3.1989 0.0218 sec/batch\n",
      "Epoch 1/1  Iteration 1755/1786 Training loss: 3.1988 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1756/1786 Training loss: 3.1987 0.0232 sec/batch\n",
      "Epoch 1/1  Iteration 1757/1786 Training loss: 3.1986 0.0266 sec/batch\n",
      "Epoch 1/1  Iteration 1758/1786 Training loss: 3.1985 0.0286 sec/batch\n",
      "Epoch 1/1  Iteration 1759/1786 Training loss: 3.1984 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 1760/1786 Training loss: 3.1983 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1761/1786 Training loss: 3.1982 0.0251 sec/batch\n",
      "Epoch 1/1  Iteration 1762/1786 Training loss: 3.1981 0.0259 sec/batch\n",
      "Epoch 1/1  Iteration 1763/1786 Training loss: 3.1980 0.0280 sec/batch\n",
      "Epoch 1/1  Iteration 1764/1786 Training loss: 3.1979 0.0258 sec/batch\n",
      "Epoch 1/1  Iteration 1765/1786 Training loss: 3.1978 0.0270 sec/batch\n",
      "Epoch 1/1  Iteration 1766/1786 Training loss: 3.1977 0.0208 sec/batch\n",
      "Epoch 1/1  Iteration 1767/1786 Training loss: 3.1976 0.0234 sec/batch\n",
      "Epoch 1/1  Iteration 1768/1786 Training loss: 3.1975 0.0287 sec/batch\n",
      "Epoch 1/1  Iteration 1769/1786 Training loss: 3.1974 0.0300 sec/batch\n",
      "Epoch 1/1  Iteration 1770/1786 Training loss: 3.1973 0.0272 sec/batch\n",
      "Epoch 1/1  Iteration 1771/1786 Training loss: 3.1972 0.0312 sec/batch\n",
      "Epoch 1/1  Iteration 1772/1786 Training loss: 3.1971 0.0315 sec/batch\n",
      "Epoch 1/1  Iteration 1773/1786 Training loss: 3.1970 0.0263 sec/batch\n",
      "Epoch 1/1  Iteration 1774/1786 Training loss: 3.1968 0.0314 sec/batch\n",
      "Epoch 1/1  Iteration 1775/1786 Training loss: 3.1967 0.0316 sec/batch\n",
      "Epoch 1/1  Iteration 1776/1786 Training loss: 3.1966 0.0291 sec/batch\n",
      "Epoch 1/1  Iteration 1777/1786 Training loss: 3.1965 0.0296 sec/batch\n",
      "Epoch 1/1  Iteration 1778/1786 Training loss: 3.1963 0.0293 sec/batch\n",
      "Epoch 1/1  Iteration 1779/1786 Training loss: 3.1962 0.0299 sec/batch\n",
      "Epoch 1/1  Iteration 1780/1786 Training loss: 3.1961 0.0273 sec/batch\n",
      "Epoch 1/1  Iteration 1781/1786 Training loss: 3.1960 0.0256 sec/batch\n",
      "Epoch 1/1  Iteration 1782/1786 Training loss: 3.1959 0.0282 sec/batch\n",
      "Epoch 1/1  Iteration 1783/1786 Training loss: 3.1958 0.0227 sec/batch\n",
      "Epoch 1/1  Iteration 1784/1786 Training loss: 3.1957 0.0242 sec/batch\n",
      "Epoch 1/1  Iteration 1785/1786 Training loss: 3.1956 0.0319 sec/batch\n",
      "Epoch 1/1  Iteration 1786/1786 Training loss: 3.1955 0.0251 sec/batch\n",
      "Validation loss: 2.89628 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                #saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i178_l512_2.444.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i178_l512_2.444.ckpt\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_8 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_8/tensor_names, save/RestoreV2_8/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_8', defined at:\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-52-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_8 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_8/tensor_names, save/RestoreV2_8/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_8 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_8/tensor_names, save/RestoreV2_8/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-91ad82b46673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/anna/i3560_l512_1.122.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-a7ae04af7e97>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_8 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_8/tensor_names, save/RestoreV2_8/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_8', defined at:\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-52-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_8 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_8/tensor_names, save/RestoreV2_8/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
