{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 45, 20, 52, 36, 47, 18, 23, 50, 29, 29, 29, 76, 20, 52, 52, 31,\n",
       "       23,  4, 20, 55, 30, 66, 30, 47,  0, 23, 20, 18, 47, 23, 20, 66, 66,\n",
       "       23, 20, 66, 30, 75, 47, 51, 23, 47, 82, 47, 18, 31, 23, 32, 41, 45,\n",
       "       20, 52, 52, 31, 23,  4, 20, 55, 30, 66, 31, 23, 30,  0, 23, 32, 41,\n",
       "       45, 20, 52, 52, 31, 23, 30, 41, 23, 30, 36,  0, 23, 40, 11, 41, 29,\n",
       "       11, 20, 31, 79, 29, 29, 56, 82, 47, 18, 31, 36, 45, 30, 41], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49, 45, 20, 52, 36, 47, 18, 23, 50, 29],\n",
       "       [25, 41, 65, 23, 45, 47, 23, 55, 40, 82],\n",
       "       [23, 46, 20, 36, 46, 45, 30, 41, 78, 23],\n",
       "       [40, 36, 45, 47, 18, 23, 11, 40, 32, 66],\n",
       "       [23, 36, 45, 47, 23, 66, 20, 41, 65, 13],\n",
       "       [23, 80, 45, 18, 40, 32, 78, 45, 23, 66],\n",
       "       [36, 23, 36, 40, 29, 65, 40, 79, 29, 29],\n",
       "       [40, 23, 45, 47, 18,  0, 47, 66,  4, 21],\n",
       "       [45, 20, 36, 23, 30,  0, 23, 36, 45, 47],\n",
       "       [47, 18,  0, 47, 66,  4, 23, 20, 41, 65]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    \n",
    "    seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                           name='softmax_w')\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4193 6.4055 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3828 7.7988 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.2458 11.0241 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.5858 7.1246 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 4.4761 6.7093 sec/batch\n",
      "Epoch 1/1  Iteration 6/178 Training loss: 4.3742 7.7595 sec/batch\n",
      "Epoch 1/1  Iteration 7/178 Training loss: 4.2791 8.1713 sec/batch\n",
      "Epoch 1/1  Iteration 8/178 Training loss: 4.1879 8.1290 sec/batch\n",
      "Epoch 1/1  Iteration 9/178 Training loss: 4.1067 7.9660 sec/batch\n",
      "Epoch 1/1  Iteration 10/178 Training loss: 4.0382 7.9437 sec/batch\n",
      "Epoch 1/1  Iteration 11/178 Training loss: 3.9791 7.2249 sec/batch\n",
      "Epoch 1/1  Iteration 12/178 Training loss: 3.9295 7.2918 sec/batch\n",
      "Epoch 1/1  Iteration 13/178 Training loss: 3.8848 7.2387 sec/batch\n",
      "Epoch 1/1  Iteration 14/178 Training loss: 3.8472 7.3928 sec/batch\n",
      "Epoch 1/1  Iteration 15/178 Training loss: 3.8112 7.3066 sec/batch\n",
      "Epoch 1/1  Iteration 16/178 Training loss: 3.7793 7.4713 sec/batch\n",
      "Epoch 1/1  Iteration 17/178 Training loss: 3.7504 8.6742 sec/batch\n",
      "Epoch 1/1  Iteration 18/178 Training loss: 3.7255 9.3761 sec/batch\n",
      "Epoch 1/1  Iteration 19/178 Training loss: 3.7024 7.8776 sec/batch\n",
      "Epoch 1/1  Iteration 20/178 Training loss: 3.6796 8.1529 sec/batch\n",
      "Epoch 1/1  Iteration 21/178 Training loss: 3.6598 8.1814 sec/batch\n",
      "Epoch 1/1  Iteration 22/178 Training loss: 3.6416 9.8486 sec/batch\n",
      "Epoch 1/1  Iteration 23/178 Training loss: 3.6242 9.5175 sec/batch\n",
      "Epoch 1/1  Iteration 24/178 Training loss: 3.6079 8.1264 sec/batch\n",
      "Epoch 1/1  Iteration 25/178 Training loss: 3.5923 9.5778 sec/batch\n",
      "Epoch 1/1  Iteration 26/178 Training loss: 3.5784 10.3667 sec/batch\n",
      "Epoch 1/1  Iteration 27/178 Training loss: 3.5660 8.7959 sec/batch\n",
      "Epoch 1/1  Iteration 28/178 Training loss: 3.5533 8.7836 sec/batch\n",
      "Epoch 1/1  Iteration 29/178 Training loss: 3.5416 9.1071 sec/batch\n",
      "Epoch 1/1  Iteration 30/178 Training loss: 3.5307 8.2537 sec/batch\n",
      "Epoch 1/1  Iteration 31/178 Training loss: 3.5212 7.4996 sec/batch\n",
      "Epoch 1/1  Iteration 32/178 Training loss: 3.5112 7.5310 sec/batch\n",
      "Epoch 1/1  Iteration 33/178 Training loss: 3.5013 8.0323 sec/batch\n",
      "Epoch 1/1  Iteration 34/178 Training loss: 3.4928 7.8676 sec/batch\n",
      "Epoch 1/1  Iteration 35/178 Training loss: 3.4842 9.1602 sec/batch\n",
      "Epoch 1/1  Iteration 36/178 Training loss: 3.4766 10.5930 sec/batch\n",
      "Epoch 1/1  Iteration 37/178 Training loss: 3.4683 8.4376 sec/batch\n",
      "Epoch 1/1  Iteration 38/178 Training loss: 3.4606 7.7546 sec/batch\n",
      "Epoch 1/1  Iteration 39/178 Training loss: 3.4531 7.6901 sec/batch\n",
      "Epoch 1/1  Iteration 40/178 Training loss: 3.4461 7.5322 sec/batch\n",
      "Epoch 1/1  Iteration 41/178 Training loss: 3.4392 7.6054 sec/batch\n",
      "Epoch 1/1  Iteration 42/178 Training loss: 3.4329 7.6626 sec/batch\n",
      "Epoch 1/1  Iteration 43/178 Training loss: 3.4265 8.0864 sec/batch\n",
      "Epoch 1/1  Iteration 44/178 Training loss: 3.4205 7.7671 sec/batch\n",
      "Epoch 1/1  Iteration 45/178 Training loss: 3.4145 7.9225 sec/batch\n",
      "Epoch 1/1  Iteration 46/178 Training loss: 3.4091 9.7364 sec/batch\n",
      "Epoch 1/1  Iteration 47/178 Training loss: 3.4040 8.1451 sec/batch\n",
      "Epoch 1/1  Iteration 48/178 Training loss: 3.3993 7.6802 sec/batch\n",
      "Epoch 1/1  Iteration 49/178 Training loss: 3.3946 7.6149 sec/batch\n",
      "Epoch 1/1  Iteration 50/178 Training loss: 3.3900 8.2011 sec/batch\n",
      "Epoch 1/1  Iteration 51/178 Training loss: 3.3855 7.6849 sec/batch\n",
      "Epoch 1/1  Iteration 52/178 Training loss: 3.3810 8.1091 sec/batch\n",
      "Epoch 1/1  Iteration 53/178 Training loss: 3.3768 7.7464 sec/batch\n",
      "Epoch 1/1  Iteration 54/178 Training loss: 3.3725 7.7446 sec/batch\n",
      "Epoch 1/1  Iteration 55/178 Training loss: 3.3686 7.6645 sec/batch\n",
      "Epoch 1/1  Iteration 56/178 Training loss: 3.3645 7.6621 sec/batch\n",
      "Epoch 1/1  Iteration 57/178 Training loss: 3.3605 7.7706 sec/batch\n",
      "Epoch 1/1  Iteration 58/178 Training loss: 3.3568 7.8684 sec/batch\n",
      "Epoch 1/1  Iteration 59/178 Training loss: 3.3530 7.7635 sec/batch\n",
      "Epoch 1/1  Iteration 60/178 Training loss: 3.3495 7.6178 sec/batch\n",
      "Epoch 1/1  Iteration 61/178 Training loss: 3.3461 7.7908 sec/batch\n",
      "Epoch 1/1  Iteration 62/178 Training loss: 3.3431 7.6625 sec/batch\n",
      "Epoch 1/1  Iteration 63/178 Training loss: 3.3402 7.6685 sec/batch\n",
      "Epoch 1/1  Iteration 64/178 Training loss: 3.3368 7.6545 sec/batch\n",
      "Epoch 1/1  Iteration 65/178 Training loss: 3.3336 7.6951 sec/batch\n",
      "Epoch 1/1  Iteration 66/178 Training loss: 3.3308 7.7156 sec/batch\n",
      "Epoch 1/1  Iteration 67/178 Training loss: 3.3281 7.7111 sec/batch\n",
      "Epoch 1/1  Iteration 68/178 Training loss: 3.3246 7.5497 sec/batch\n",
      "Epoch 1/1  Iteration 69/178 Training loss: 3.3216 8.2958 sec/batch\n",
      "Epoch 1/1  Iteration 70/178 Training loss: 3.3192 9.4872 sec/batch\n",
      "Epoch 1/1  Iteration 71/178 Training loss: 3.3164 12.4722 sec/batch\n",
      "Epoch 1/1  Iteration 72/178 Training loss: 3.3140 9.9192 sec/batch\n",
      "Epoch 1/1  Iteration 73/178 Training loss: 3.3114 11.4037 sec/batch\n",
      "Epoch 1/1  Iteration 74/178 Training loss: 3.3088 11.6353 sec/batch\n",
      "Epoch 1/1  Iteration 75/178 Training loss: 3.3064 8.0104 sec/batch\n",
      "Epoch 1/1  Iteration 76/178 Training loss: 3.3042 8.7804 sec/batch\n",
      "Epoch 1/1  Iteration 77/178 Training loss: 3.3019 8.1942 sec/batch\n",
      "Epoch 1/1  Iteration 78/178 Training loss: 3.2996 7.5201 sec/batch\n",
      "Epoch 1/1  Iteration 79/178 Training loss: 3.2972 7.1514 sec/batch\n",
      "Epoch 1/1  Iteration 80/178 Training loss: 3.2948 7.0735 sec/batch\n",
      "Epoch 1/1  Iteration 81/178 Training loss: 3.2925 7.7670 sec/batch\n",
      "Epoch 1/1  Iteration 82/178 Training loss: 3.2904 8.9347 sec/batch\n",
      "Epoch 1/1  Iteration 83/178 Training loss: 3.2883 8.1125 sec/batch\n",
      "Epoch 1/1  Iteration 84/178 Training loss: 3.2862 8.4955 sec/batch\n",
      "Epoch 1/1  Iteration 85/178 Training loss: 3.2838 7.7691 sec/batch\n",
      "Epoch 1/1  Iteration 86/178 Training loss: 3.2815 7.8930 sec/batch\n",
      "Epoch 1/1  Iteration 87/178 Training loss: 3.2793 10.1977 sec/batch\n",
      "Epoch 1/1  Iteration 88/178 Training loss: 3.2770 9.4918 sec/batch\n",
      "Epoch 1/1  Iteration 89/178 Training loss: 3.2751 7.8042 sec/batch\n",
      "Epoch 1/1  Iteration 90/178 Training loss: 3.2731 9.0693 sec/batch\n",
      "Epoch 1/1  Iteration 91/178 Training loss: 3.2710 7.8789 sec/batch\n",
      "Epoch 1/1  Iteration 92/178 Training loss: 3.2690 7.7337 sec/batch\n",
      "Epoch 1/1  Iteration 93/178 Training loss: 3.2669 7.7709 sec/batch\n",
      "Epoch 1/1  Iteration 94/178 Training loss: 3.2651 7.7133 sec/batch\n",
      "Epoch 1/1  Iteration 95/178 Training loss: 3.2632 7.7003 sec/batch\n",
      "Epoch 1/1  Iteration 96/178 Training loss: 3.2614 7.7482 sec/batch\n",
      "Epoch 1/1  Iteration 97/178 Training loss: 3.2598 7.7917 sec/batch\n",
      "Epoch 1/1  Iteration 98/178 Training loss: 3.2579 7.6570 sec/batch\n",
      "Epoch 1/1  Iteration 99/178 Training loss: 3.2562 7.8879 sec/batch\n",
      "Epoch 1/1  Iteration 100/178 Training loss: 3.2544 7.6365 sec/batch\n",
      "Epoch 1/1  Iteration 101/178 Training loss: 3.2527 7.7496 sec/batch\n",
      "Epoch 1/1  Iteration 102/178 Training loss: 3.2510 8.0219 sec/batch\n",
      "Epoch 1/1  Iteration 103/178 Training loss: 3.2494 7.6639 sec/batch\n",
      "Epoch 1/1  Iteration 104/178 Training loss: 3.2476 7.7298 sec/batch\n",
      "Epoch 1/1  Iteration 105/178 Training loss: 3.2458 7.9683 sec/batch\n",
      "Epoch 1/1  Iteration 106/178 Training loss: 3.2441 7.9396 sec/batch\n",
      "Epoch 1/1  Iteration 107/178 Training loss: 3.2422 7.7319 sec/batch\n",
      "Epoch 1/1  Iteration 108/178 Training loss: 3.2403 7.8678 sec/batch\n",
      "Epoch 1/1  Iteration 109/178 Training loss: 3.2386 7.7756 sec/batch\n",
      "Epoch 1/1  Iteration 110/178 Training loss: 3.2366 7.8386 sec/batch\n",
      "Epoch 1/1  Iteration 111/178 Training loss: 3.2347 7.8144 sec/batch\n",
      "Epoch 1/1  Iteration 112/178 Training loss: 3.2329 8.0464 sec/batch\n",
      "Epoch 1/1  Iteration 113/178 Training loss: 3.2310 7.7191 sec/batch\n",
      "Epoch 1/1  Iteration 114/178 Training loss: 3.2289 7.8095 sec/batch\n",
      "Epoch 1/1  Iteration 115/178 Training loss: 3.2269 7.7212 sec/batch\n",
      "Epoch 1/1  Iteration 116/178 Training loss: 3.2248 7.7518 sec/batch\n",
      "Epoch 1/1  Iteration 117/178 Training loss: 3.2228 7.7926 sec/batch\n",
      "Epoch 1/1  Iteration 118/178 Training loss: 3.2208 8.6615 sec/batch\n",
      "Epoch 1/1  Iteration 119/178 Training loss: 3.2190 7.8011 sec/batch\n",
      "Epoch 1/1  Iteration 120/178 Training loss: 3.2171 7.7793 sec/batch\n",
      "Epoch 1/1  Iteration 121/178 Training loss: 3.2153 7.8075 sec/batch\n",
      "Epoch 1/1  Iteration 122/178 Training loss: 3.2133 7.7094 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 123/178 Training loss: 3.2114 7.6833 sec/batch\n",
      "Epoch 1/1  Iteration 124/178 Training loss: 3.2096 7.6943 sec/batch\n",
      "Epoch 1/1  Iteration 125/178 Training loss: 3.2075 7.7939 sec/batch\n",
      "Epoch 1/1  Iteration 126/178 Training loss: 3.2053 8.1154 sec/batch\n",
      "Epoch 1/1  Iteration 127/178 Training loss: 3.2033 7.7295 sec/batch\n",
      "Epoch 1/1  Iteration 128/178 Training loss: 3.2012 8.1357 sec/batch\n",
      "Epoch 1/1  Iteration 129/178 Training loss: 3.1990 9.0941 sec/batch\n",
      "Epoch 1/1  Iteration 130/178 Training loss: 3.1967 7.7923 sec/batch\n",
      "Epoch 1/1  Iteration 131/178 Training loss: 3.1945 7.9559 sec/batch\n",
      "Epoch 1/1  Iteration 132/178 Training loss: 3.1921 7.8508 sec/batch\n",
      "Epoch 1/1  Iteration 133/178 Training loss: 3.1899 7.7370 sec/batch\n",
      "Epoch 1/1  Iteration 134/178 Training loss: 3.1876 7.8447 sec/batch\n",
      "Epoch 1/1  Iteration 135/178 Training loss: 3.1851 7.8307 sec/batch\n",
      "Epoch 1/1  Iteration 136/178 Training loss: 3.1825 7.6987 sec/batch\n",
      "Epoch 1/1  Iteration 137/178 Training loss: 3.1801 7.7353 sec/batch\n",
      "Epoch 1/1  Iteration 138/178 Training loss: 3.1776 7.9418 sec/batch\n",
      "Epoch 1/1  Iteration 139/178 Training loss: 3.1752 8.8331 sec/batch\n",
      "Epoch 1/1  Iteration 140/178 Training loss: 3.1728 7.7622 sec/batch\n",
      "Epoch 1/1  Iteration 141/178 Training loss: 3.1703 7.7218 sec/batch\n",
      "Epoch 1/1  Iteration 142/178 Training loss: 3.1678 7.7149 sec/batch\n",
      "Epoch 1/1  Iteration 143/178 Training loss: 3.1653 7.7378 sec/batch\n",
      "Epoch 1/1  Iteration 144/178 Training loss: 3.1628 7.7089 sec/batch\n",
      "Epoch 1/1  Iteration 145/178 Training loss: 3.1603 7.6942 sec/batch\n",
      "Epoch 1/1  Iteration 146/178 Training loss: 3.1578 7.8350 sec/batch\n",
      "Epoch 1/1  Iteration 147/178 Training loss: 3.1553 7.8159 sec/batch\n",
      "Epoch 1/1  Iteration 148/178 Training loss: 3.1530 7.7315 sec/batch\n",
      "Epoch 1/1  Iteration 149/178 Training loss: 3.1502 7.7150 sec/batch\n",
      "Epoch 1/1  Iteration 150/178 Training loss: 3.1475 7.7548 sec/batch\n",
      "Epoch 1/1  Iteration 151/178 Training loss: 3.1451 7.6693 sec/batch\n",
      "Epoch 1/1  Iteration 152/178 Training loss: 3.1426 7.8626 sec/batch\n",
      "Epoch 1/1  Iteration 153/178 Training loss: 3.1399 7.7582 sec/batch\n",
      "Epoch 1/1  Iteration 154/178 Training loss: 3.1373 7.7366 sec/batch\n",
      "Epoch 1/1  Iteration 155/178 Training loss: 3.1345 7.6961 sec/batch\n",
      "Epoch 1/1  Iteration 156/178 Training loss: 3.1317 9.0133 sec/batch\n",
      "Epoch 1/1  Iteration 157/178 Training loss: 3.1288 8.5374 sec/batch\n",
      "Epoch 1/1  Iteration 158/178 Training loss: 3.1260 8.1346 sec/batch\n",
      "Epoch 1/1  Iteration 159/178 Training loss: 3.1230 7.7271 sec/batch\n",
      "Epoch 1/1  Iteration 160/178 Training loss: 3.1202 7.8432 sec/batch\n",
      "Epoch 1/1  Iteration 161/178 Training loss: 3.1173 7.8405 sec/batch\n",
      "Epoch 1/1  Iteration 162/178 Training loss: 3.1143 7.8984 sec/batch\n",
      "Epoch 1/1  Iteration 163/178 Training loss: 3.1112 7.7751 sec/batch\n",
      "Epoch 1/1  Iteration 164/178 Training loss: 3.1082 7.7840 sec/batch\n",
      "Epoch 1/1  Iteration 165/178 Training loss: 3.1053 7.9575 sec/batch\n",
      "Epoch 1/1  Iteration 166/178 Training loss: 3.1023 7.9242 sec/batch\n",
      "Epoch 1/1  Iteration 167/178 Training loss: 3.0994 7.7570 sec/batch\n",
      "Epoch 1/1  Iteration 168/178 Training loss: 3.0964 7.6998 sec/batch\n",
      "Epoch 1/1  Iteration 169/178 Training loss: 3.0935 7.6811 sec/batch\n",
      "Epoch 1/1  Iteration 170/178 Training loss: 3.0904 7.7920 sec/batch\n",
      "Epoch 1/1  Iteration 171/178 Training loss: 3.0875 7.7308 sec/batch\n",
      "Epoch 1/1  Iteration 172/178 Training loss: 3.0846 7.8421 sec/batch\n",
      "Epoch 1/1  Iteration 173/178 Training loss: 3.0819 7.7761 sec/batch\n",
      "Epoch 1/1  Iteration 174/178 Training loss: 3.0791 7.7388 sec/batch\n",
      "Epoch 1/1  Iteration 175/178 Training loss: 3.0765 7.8302 sec/batch\n",
      "Epoch 1/1  Iteration 176/178 Training loss: 3.0737 7.7359 sec/batch\n",
      "Epoch 1/1  Iteration 177/178 Training loss: 3.0707 8.5002 sec/batch\n",
      "Epoch 1/1  Iteration 178/178 Training loss: 3.0676 7.8285 sec/batch\n",
      "Validation loss: 2.44407 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i178_l512_2.444.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i178_l512_2.444.ckpt\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_19', defined at:\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-17-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-91ad82b46673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/anna/i3560_l512_1.122.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a7ae04af7e97>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_19', defined at:\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-17-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/nusco/Applications/anaconda3/envs/tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
